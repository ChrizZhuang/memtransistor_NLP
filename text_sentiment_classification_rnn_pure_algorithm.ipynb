{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrizZhuang/memtransistor_NLP/blob/main/text_sentiment_classification_rnn_pure_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71346a93-c8d1-4f54-8fa9-f65202680789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2IXX0M/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2IXX0M/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2IXX0M/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vd4_BGKyurao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23306f1e-e0fd-435d-cd03-d63b393e0b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n",
            "text:  b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n",
            "label:  0\n",
            "text:  b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'\n",
            "label:  0\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(3):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(type(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BliOc7dNhVb4",
        "outputId": "a6dd8c6f-ed6e-407a-d9cb-a4efbe92ff7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jqkvdcFv41wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c90ef3a-16a9-4070-b31c-7742cd067357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b\"The first one was the best. The second one sucked because the dialog was terrible. Although, the storyline wasn't so bad (in fact, all story lines are good and bad). Throughout the movie, I dosed off a few times. I know that Jackie Chan is a great martial arts expertise, but not a good actor in Rush Hour 2. Chris Tucker, too, wasn't good. And Zhang Ziyi, what can I say, a few lines, terrible acting (But that's based on her script). All the characters there were not that good. But, some of the things I like in Rush Hour 2 is always the action and less sex scenes. I know that Jackie Chan doesn't do those things which is good for him.\"\n",
            " b'This movie explores the difficulties that strain hopes, dreams, love and friendship, and incorporates humour beautifully. Along with a stunning cast and brilliant filming, the sound track enhances and amplifies the atmosphere and mood of this work of art. All actors and actresses give an extremely good performance, surpassing expectation in every way. Parminder Nagra is brought on to the big screen for the first time in this film, and she is exceptional, capturing the vividness and vitality that this movie is all about. Keira Knightly also works well with her co-stars, and this is her best work so far.<br /><br />All in all, this is brilliant film, and one that everyone should make the effort to see at least once.'\n",
            " b'When a movie of a book seems pointless and incomprehensible, the cause can invariably be found in the book: either it was pointless to start with, or the point is one not easily conveyed to film, or the movie missed the point, which is the most frequent of these results, and the easiest to happen, especially when the point is one not easily defined. The book \"Morvern Callar\" has a point; every reader of the book must have felt this, and felt as if he had gotten it; but I suspect most of them could not state it in words. I\\'m not sure I can, myself, but perhaps it comes to this, or something like it: Things come, things go, such is life, but we carry on; or at any rate some of us--people like Morvern--do. No doubt a more erudite critic could construct a more adequate definition. But the important fact is that there is a point--possibly the sum of the entire story is the point--and that this would have been the main thing to keep in view, and to carry over, in adapting the story to film. The maker of this film evidently missed the point, and doesn\\'t substitute one of her own; and so the film is about nothing.<br /><br />This is not the usual complaint of a book-lover that his favorite text has been violated. The merit of the book is something I conceded grudgingly: in reading it I found it a bloody nuisance, and an occasion for kicking the author in the pants and getting him in to finish the job properly. The narrative is supposed to be the work of the half-educated Morvern, but that illusion is constantly dispelled by a dozen different types of literary effect, as if the author were poking at her with his pen; there are inconsistencies of style and tone, as if different sections had been composed at different times; and any conclusions I could reach about Morvern had to remain tentative because it was uncertain which implications the author intended and which he did not: for instance, despite Morvern\\'s own self-characterization as a raver, am I wrong that in the end she remains essentially a working-class Scots girl, and beneath her wrapping of music downloads not so different from those of generations past? In any case, despite my irritation at the author, I couldn\\'t deny that his book stuck with me; and what I couldn\\'t get out of my head was his character\\'s attitude, her angle on the world, which was almost as vivid as a Goya portrait. Morvern is the kind of person who\\'s always encountering situations at once rather comic and rather horrible; occasionally she invites them but more often they land on her, like flies, so that much of her life consists of a kind of gauche but graceful slogging-through, unconsciously practical and unconsciously philosophical--and that doesn\\'t begin to describe it idiosyncratically enough. The complex of incidents and of Morvern\\'s responses to them are the substance of the book, and its achievement, in exposing a cross-section of existence it would be difficult to illuminate otherwise; for all my dislike of the book, I can see this.<br /><br />The Morvern just described is not the Morvern of the movie; or if it is, most of her is kept offscreen. An actress who might have been a good fit for the character, had she been the right age at the right time, is Angharad Rees, from the old TV series \"Poldark\". Samantha Morton, then, would seem like good casting: she\\'s rather the same sort of actress, and in one of her earlier movies, \"Jesus\\' Son\", she played a girl who with a few adjustments could have been turned into this one. Unfortunately, as the film turned out, she doesn\\'t have the character from the book to play. For one thing, the book is one that, if it is to be dramatized, virtually cries out for monologues by the main character to the audience; without her comments, her perspective, her voice, the story loses most of its meaning. It has lost more of it in that the adaptor has expurgated it of its comic and horrible elements: the most memorable incidents from the book are curtailed before they turn grotty, and so Morvern\\'s responses (whether of amusement or distaste, depending on her mood) are missing too, and the incidents no longer have a reason for being in the story. In short, the filmmaker chose for some reason to turn a brisk, edgy serio-comic novel into a genteel art TV film, and chose as her typical image one of Ms. Morton languishing in a artistically shaded melancholy; as if the outing Morvern signs up for were a tour of the Stations of the Cross. This isn\\'t at all what the book, or the Morvern of the book, was about. For another thing, the Morvern of the movie isn\\'t Scottish (the actress said in an interview she hadn\\'t had time to study up the accent), and she ought to be: it\\'s important that she, her family, and her mates are all from a single place. And finally the film is missing the end of the story: Morvern\\'s spending all she has and coming home to icy darkness: it\\'s winter, the dam has frozen, the power has gone out, and the pub is dark. Minus this, and minus all of the rest, what\\'s left is a failed art film, a dead film, about a subject whose strength lay precisely in her refusal, or native inability, ever to give in to being dead.']\n",
            "\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "\n",
            "labels:  [0 1 0]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print(type(example)) # type of example: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "# Maximum size of the vocabulary for this layer. \n",
        "# This should only be specified when adapting a vocabulary or when setting pad_to_max_tokens=True. \n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "# adapt: Fits the state of the preprocessing layer to the data being passed.\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e11d07-6848-4d21-daf5-b58ab234f316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "print(len(vocab)) # len(vocab) = 1000 as VOCAB_SIZE = 1000\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b711d1d-885b-4958-d173-f517257ad7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "995\n",
            "995\n",
            "995\n"
          ]
        }
      ],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "for i in range(3):\n",
        "  print(len(encoded_example[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N_tD0QY5wXaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d69303-3f64-484a-b56f-988b05c65d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"The first one was the best. The second one sucked because the dialog was terrible. Although, the storyline wasn't so bad (in fact, all story lines are good and bad). Throughout the movie, I dosed off a few times. I know that Jackie Chan is a great martial arts expertise, but not a good actor in Rush Hour 2. Chris Tucker, too, wasn't good. And Zhang Ziyi, what can I say, a few lines, terrible acting (But that's based on her script). All the characters there were not that good. But, some of the things I like in Rush Hour 2 is always the action and less sex scenes. I know that Jackie Chan doesn't do those things which is good for him.\"\n",
            "Round-trip:  the first one was the best the second one [UNK] because the dialog was terrible although the storyline wasnt so bad in fact all story lines are good and bad throughout the movie i [UNK] off a few times i know that [UNK] [UNK] is a great [UNK] [UNK] [UNK] but not a good actor in [UNK] hour 2 [UNK] [UNK] too wasnt good and [UNK] [UNK] what can i say a few lines terrible acting but thats based on her script all the characters there were not that good but some of the things i like in [UNK] hour 2 is always the action and less sex scenes i know that [UNK] [UNK] doesnt do those things which is good for him                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "\n",
            "Original:  b'This movie explores the difficulties that strain hopes, dreams, love and friendship, and incorporates humour beautifully. Along with a stunning cast and brilliant filming, the sound track enhances and amplifies the atmosphere and mood of this work of art. All actors and actresses give an extremely good performance, surpassing expectation in every way. Parminder Nagra is brought on to the big screen for the first time in this film, and she is exceptional, capturing the vividness and vitality that this movie is all about. Keira Knightly also works well with her co-stars, and this is her best work so far.<br /><br />All in all, this is brilliant film, and one that everyone should make the effort to see at least once.'\n",
            "Round-trip:  this movie [UNK] the [UNK] that [UNK] [UNK] [UNK] love and [UNK] and [UNK] [UNK] [UNK] along with a [UNK] cast and brilliant [UNK] the sound [UNK] [UNK] and [UNK] the atmosphere and [UNK] of this work of art all actors and [UNK] give an extremely good performance [UNK] [UNK] in every way [UNK] [UNK] is brought on to the big screen for the first time in this film and she is [UNK] [UNK] the [UNK] and [UNK] that this movie is all about [UNK] [UNK] also works well with her [UNK] and this is her best work so [UNK] br all in all this is brilliant film and one that everyone should make the effort to see at least once                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "\n",
            "Original:  b'When a movie of a book seems pointless and incomprehensible, the cause can invariably be found in the book: either it was pointless to start with, or the point is one not easily conveyed to film, or the movie missed the point, which is the most frequent of these results, and the easiest to happen, especially when the point is one not easily defined. The book \"Morvern Callar\" has a point; every reader of the book must have felt this, and felt as if he had gotten it; but I suspect most of them could not state it in words. I\\'m not sure I can, myself, but perhaps it comes to this, or something like it: Things come, things go, such is life, but we carry on; or at any rate some of us--people like Morvern--do. No doubt a more erudite critic could construct a more adequate definition. But the important fact is that there is a point--possibly the sum of the entire story is the point--and that this would have been the main thing to keep in view, and to carry over, in adapting the story to film. The maker of this film evidently missed the point, and doesn\\'t substitute one of her own; and so the film is about nothing.<br /><br />This is not the usual complaint of a book-lover that his favorite text has been violated. The merit of the book is something I conceded grudgingly: in reading it I found it a bloody nuisance, and an occasion for kicking the author in the pants and getting him in to finish the job properly. The narrative is supposed to be the work of the half-educated Morvern, but that illusion is constantly dispelled by a dozen different types of literary effect, as if the author were poking at her with his pen; there are inconsistencies of style and tone, as if different sections had been composed at different times; and any conclusions I could reach about Morvern had to remain tentative because it was uncertain which implications the author intended and which he did not: for instance, despite Morvern\\'s own self-characterization as a raver, am I wrong that in the end she remains essentially a working-class Scots girl, and beneath her wrapping of music downloads not so different from those of generations past? In any case, despite my irritation at the author, I couldn\\'t deny that his book stuck with me; and what I couldn\\'t get out of my head was his character\\'s attitude, her angle on the world, which was almost as vivid as a Goya portrait. Morvern is the kind of person who\\'s always encountering situations at once rather comic and rather horrible; occasionally she invites them but more often they land on her, like flies, so that much of her life consists of a kind of gauche but graceful slogging-through, unconsciously practical and unconsciously philosophical--and that doesn\\'t begin to describe it idiosyncratically enough. The complex of incidents and of Morvern\\'s responses to them are the substance of the book, and its achievement, in exposing a cross-section of existence it would be difficult to illuminate otherwise; for all my dislike of the book, I can see this.<br /><br />The Morvern just described is not the Morvern of the movie; or if it is, most of her is kept offscreen. An actress who might have been a good fit for the character, had she been the right age at the right time, is Angharad Rees, from the old TV series \"Poldark\". Samantha Morton, then, would seem like good casting: she\\'s rather the same sort of actress, and in one of her earlier movies, \"Jesus\\' Son\", she played a girl who with a few adjustments could have been turned into this one. Unfortunately, as the film turned out, she doesn\\'t have the character from the book to play. For one thing, the book is one that, if it is to be dramatized, virtually cries out for monologues by the main character to the audience; without her comments, her perspective, her voice, the story loses most of its meaning. It has lost more of it in that the adaptor has expurgated it of its comic and horrible elements: the most memorable incidents from the book are curtailed before they turn grotty, and so Morvern\\'s responses (whether of amusement or distaste, depending on her mood) are missing too, and the incidents no longer have a reason for being in the story. In short, the filmmaker chose for some reason to turn a brisk, edgy serio-comic novel into a genteel art TV film, and chose as her typical image one of Ms. Morton languishing in a artistically shaded melancholy; as if the outing Morvern signs up for were a tour of the Stations of the Cross. This isn\\'t at all what the book, or the Morvern of the book, was about. For another thing, the Morvern of the movie isn\\'t Scottish (the actress said in an interview she hadn\\'t had time to study up the accent), and she ought to be: it\\'s important that she, her family, and her mates are all from a single place. And finally the film is missing the end of the story: Morvern\\'s spending all she has and coming home to icy darkness: it\\'s winter, the dam has frozen, the power has gone out, and the pub is dark. Minus this, and minus all of the rest, what\\'s left is a failed art film, a dead film, about a subject whose strength lay precisely in her refusal, or native inability, ever to give in to being dead.'\n",
            "Round-trip:  when a movie of a book seems [UNK] and [UNK] the [UNK] can [UNK] be found in the book either it was [UNK] to start with or the point is one not easily [UNK] to film or the movie [UNK] the point which is the most [UNK] of these [UNK] and the [UNK] to happen especially when the point is one not easily [UNK] the book [UNK] [UNK] has a point every [UNK] of the book must have felt this and felt as if he had [UNK] it but i [UNK] most of them could not [UNK] it in words im not sure i can myself but perhaps it comes to this or something like it things come things go such is life but we [UNK] on or at any [UNK] some of [UNK] like [UNK] no doubt a more [UNK] [UNK] could [UNK] a more [UNK] [UNK] but the important fact is that there is a [UNK] the [UNK] of the entire story is the [UNK] that this would have been the main thing to keep in view and to [UNK] over in [UNK] the story to film the [UNK] of this film [UNK] [UNK] the point and doesnt [UNK] one of her own and so the film is about [UNK] br this is not the usual [UNK] of a [UNK] that his favorite [UNK] has been [UNK] the [UNK] of the book is something i [UNK] [UNK] in reading it i found it a [UNK] [UNK] and an [UNK] for [UNK] the [UNK] in the [UNK] and getting him in to [UNK] the job [UNK] the [UNK] is supposed to be the work of the [UNK] [UNK] but that [UNK] is [UNK] [UNK] by a [UNK] different [UNK] of [UNK] effect as if the [UNK] were [UNK] at her with his [UNK] there are [UNK] of style and [UNK] as if different [UNK] had been [UNK] at different times and any [UNK] i could [UNK] about [UNK] had to [UNK] [UNK] because it was [UNK] which [UNK] the [UNK] [UNK] and which he did not for [UNK] despite [UNK] own [UNK] as a [UNK] am i wrong that in the end she [UNK] [UNK] a [UNK] [UNK] girl and [UNK] her [UNK] of music [UNK] not so different from those of [UNK] past in any case despite my [UNK] at the [UNK] i couldnt [UNK] that his book [UNK] with me and what i couldnt get out of my head was his characters [UNK] her [UNK] on the world which was almost as [UNK] as a [UNK] [UNK] [UNK] is the kind of person whos always [UNK] [UNK] at once rather comic and rather horrible [UNK] she [UNK] them but more often they [UNK] on her like [UNK] so that much of her life [UNK] of a kind of [UNK] but [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] that doesnt begin to [UNK] it [UNK] enough the [UNK] of [UNK] and of [UNK] [UNK] to them are the [UNK] of the book and its [UNK] in [UNK] a [UNK] of [UNK] it would be difficult to [UNK] otherwise for all my [UNK] of the book i can see [UNK] br the [UNK] just [UNK] is not the [UNK] of the movie or if it is most of her is kept [UNK] an actress who might have been a good [UNK] for the character had she been the right age at the right time is [UNK] [UNK] from the old tv series [UNK] [UNK] [UNK] then would seem like good casting shes rather the same sort of actress and in one of her earlier movies [UNK] son she played a girl who with a few [UNK] could have been turned into this one unfortunately as the film turned out she doesnt have the character from the book to play for one thing the book is one that if it is to be [UNK] [UNK] [UNK] out for [UNK] by the main character to the audience without her comments her [UNK] her voice the story [UNK] most of its [UNK] it has lost more of it in that the [UNK] has [UNK] it of its comic and horrible elements the most memorable [UNK] from the book are [UNK] before they turn [UNK] and so [UNK] [UNK] whether of [UNK] or [UNK] [UNK] on her [UNK] are missing too and the [UNK] no [UNK] have a reason for being in the story in short the [UNK] [UNK] for some reason to turn a [UNK] [UNK] [UNK] novel into a [UNK] art tv film and [UNK] as her typical [UNK] one of [UNK] [UNK] [UNK] in a [UNK] [UNK] [UNK] as if the [UNK] [UNK] [UNK] up for were a [UNK] of the [UNK] of the [UNK] this isnt at all what the book or the [UNK] of the book was about for another thing the [UNK] of the movie isnt [UNK] the actress said in an [UNK] she [UNK] had time to [UNK] up the [UNK] and she [UNK] to be its important that she her family and her [UNK] are all from a single place and finally the film is missing the end of the story [UNK] [UNK] all she has and coming home to [UNK] [UNK] its [UNK] the [UNK] has [UNK] the power has gone out and the [UNK] is dark [UNK] this and [UNK] all of the rest whats left is a [UNK] art film a dead film about a subject whose [UNK] [UNK] [UNK] in her [UNK] or [UNK] [UNK] ever to give in to being dead                                                                \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "87a8-CwfKebw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9613d17-0304-43c7-eff6-582690bea7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "O41gw3KfWHus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b29428d-ddb9-444e-9213-cea8ffa7e354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00625656]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UIgpuTeFNDzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c438410-808f-4070-d98b-6b344f0b5976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00625655]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "# print(np.array([sample_text, padding]))\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw86wWS4YgR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e4c4ed-bc1e-46f9-e128-8fee09ff26b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "332/391 [========================>.....] - ETA: 9s - loss: 0.6615 - accuracy: 0.5533"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset, epochs=5,\n",
        "                   validation_data=test_dataset,\n",
        "                   validation_steps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_sentiment_classification_rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "RNN_4_terminals_memtransistor_sim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrizZhuang/memtransistor_NLP/blob/main/RNN_4_terminals_memtransistor_sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "516V8VJij8a7"
      },
      "source": [
        "#**RNN Simulation of Dual gates 4 terminals memtransistor**\n",
        "\n",
        "**Hardware paper**\n",
        "\n",
        "- *Dual-Gated MoS2 Memtransistor Crossbar Array* https://onlinelibrary.wiley.com/doi/abs/10.1002/adfm.202003683\n",
        "\n",
        "**Hardware code with the application of CV**\n",
        "\n",
        "- https://colab.research.google.com/drive/1_zY4qp1u8IZhc_ht4t-iHr2m53j7u2li\n",
        "\n",
        "**RNN Algorithm for text generation**\n",
        "\n",
        "- https://www.tensorflow.org/text/tutorials/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU0puRjbltix",
        "outputId": "b6688c54-71d2-4a82-d89d-5f4ecb28e504"
      },
      "source": [
        "###################### IMPORTS\n",
        "# uses tensorflow v2.7.0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize some constants about the device\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "# parameter set by user that gives all the possible normalized weight states\n",
        "# assumes (1) discrete number of states that are normalized \n",
        "#         (2) states are set by two synaptic devices such that weight = weight_p - weight_m\n",
        "#         (3) because of (2), weights can vary from [-1,1]\n",
        "# user input = a 1D numpy array with values from [-1, 1]\n",
        "DEVICE_STATES = np.random.normal(-1, 1, 100)\n",
        "# print(len(DEVICE_STATES))\n",
        "\n",
        "# parameters for simulating read noise\n",
        "# user input = read noise mean and standard dev assuming a normal noise function\n",
        "READ_NOISE_MEAN = 0\n",
        "READ_NOISE_STDDEV = 0.1\n",
        "\n",
        "# parameter for simulating device-to-device variation\n",
        "# user input =  standard deviation of conductances\n",
        "DEVICE_VARIATION_STDDEV = 0.1\n",
        "\n",
        "# parameter for simulating devices that get stuck on Gmax or Gmin states from the start\n",
        "# user input = probability for a device to get stuck\n",
        "DEVICE_STUCK_ON_PROB = 0.1\n",
        "DEVICE_STUCK_OFF_PROB = 0.1"
      ],
      "metadata": {
        "id": "8zTVB2RB7ykk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "# Split the text into tokens\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "# create the layer and convert each character into a numeric ID\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "# invert the numerical representation and recover to human-readable strings from it\n",
        "# maps string features to integer indices\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)# generate the characters\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy() # join the characters back into strings\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "HUqmuUPW9LPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c05346d-e6e4-49ca-e702-d95a18579392"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create training examples and targets**"
      ],
      "metadata": {
        "id": "G6uGs7QJBlXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create training examples and targets\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# convert the text vector into a stream of character indices\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) \n",
        "# for ids in ids_dataset.take(10):\n",
        "  # print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# convert individual characters to sequences of the desired size, i.e., seq_length+1.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "# for seq in sequences.take(1):\n",
        "  # print(chars_from_ids(seq))\n",
        "\n",
        "# for seq in sequences.take(5):\n",
        "  # print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "# Takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#for input_example, target_example in dataset.take(1):\n",
        "  #print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  #print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "F0dGeMMxANoF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create training data**"
      ],
      "metadata": {
        "id": "8JyKz6CDCG5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# dataset"
      ],
      "metadata": {
        "id": "VsGrXnl2CLVl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Built the Model**"
      ],
      "metadata": {
        "id": "3q-mgaKsCQ43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Define the model\n",
        "# Embedding layer:\n",
        "# Turns positive integers (indexes) into dense vectors of fixed size.\n",
        "# e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "# This layer can only be used as the first layer in a model.\n",
        "# Word embeddings provide a dense representation of words and their relative meanings.\n",
        "# They are an improvement over sparse representations used in simpler bag of word model representations.\n",
        "# Word embeddings can be learned from text data and reused among projects. \n",
        "# They can also be learned as part of fitting a neural network on text data.\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "40rCnfouCVxB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "EOzoZbJz0-94"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zj1qB5N2flY",
        "outputId": "bf73ff7b-28d5-4271-da3d-11561c906bbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get actual predictions from the model you need to sample from \n",
        "# the output distribution and actual character indices\n",
        "# Try from the first example in the batch\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "#sampled_indices\n",
        "\n",
        "# Decode these to see the text predicted by this untrained model\n",
        "#print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "#print()\n",
        "#print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "t3E-YuZY30mh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Train the model**"
      ],
      "metadata": {
        "id": "FMRkQxBr4QVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach an optimizer and a loss function\n",
        "# Set from_logits = True as the model returns logits\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Configure checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2elN1GR4V0Y",
        "outputId": "a2aeb6f7-887b-49d8-ebba-fa9dd173a001"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.189085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrXboxFX5tKD",
        "outputId": "60835671-9001-4bf8-b966-f9db37ee614b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 885s 5s/step - loss: 2.7171\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 825s 5s/step - loss: 1.9890\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 823s 5s/step - loss: 1.7064\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 903s 5s/step - loss: 1.5465\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 880s 5s/step - loss: 1.4487\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 908s 5s/step - loss: 1.3818\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 817s 5s/step - loss: 1.3288\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 820s 5s/step - loss: 1.2833\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 818s 5s/step - loss: 1.2410\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 816s 5s/step - loss: 1.2009\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 811s 5s/step - loss: 1.1618\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 811s 5s/step - loss: 1.1190\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 815s 5s/step - loss: 1.0755\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 813s 5s/step - loss: 1.0301\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 810s 5s/step - loss: 0.9798\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 809s 5s/step - loss: 0.9291\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.8764\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.8244\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.7728\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 820s 5s/step - loss: 0.7240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Make a one-step prediction**"
      ],
      "metadata": {
        "id": "550mMKto7Rbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "neOoACoK7coL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Run a loop to generate some text\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsITLP527jzM",
        "outputId": "13477402-36ac-4a1c-a682-f32241c7ecc4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "\n",
            "Third Citizen:\n",
            "No, pray you, do you yet.\n",
            "\n",
            "CORIOLANUS:\n",
            "Come, you shall no.\n",
            "\n",
            "PAULINA:\n",
            "A pine;\n",
            "I will mourn his house and pecks;\n",
            "Which lanes, dwelling wanton fool, devils,\n",
            "For mountay stones, than attend more\n",
            "Than when the rabs will hence in arms.\n",
            "\n",
            "MARCIUS:\n",
            "They have a daint.\n",
            "\n",
            "First Senator:\n",
            "Grief she would meet her.\n",
            "\n",
            "PAULINA:\n",
            "These eyes,\n",
            "Yet wouldst as kneel of my old misury.\n",
            "\n",
            "ROMEO:\n",
            "When the coats speak? When give us both may call her be\n",
            "The father's banish'd Hell, when he wrinkless\n",
            "To bear clouds with you.\n",
            "\n",
            "Brother:\n",
            "What, that's a hope.\n",
            "\n",
            "MARCIUS:\n",
            "May you think it, speak any dron?\n",
            "\n",
            "BRUTUS:\n",
            "Let's ha't me make thee then a wisse: but\n",
            "For somement with no short rest of the world:\n",
            "This was hold you this but two enemies,\n",
            "Mark and proclaiming hands than will hold a\n",
            "Oxfording to this place. Now, by good Paul,\n",
            "Was my jewel and be hance to such mejeated with\n",
            "As blind forth an affects would dispatch'd.\n",
            "\n",
            "MENENIUS:\n",
            "And 'Alf Lord:\n",
            "Well, you lament; out for oy\n",
            "That doth fagned already have mine enem \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.21230149269104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Functions for memtransistor simulation**\n",
        "\n",
        "Memtransistor has effects on weight update mechanism and thereby influence the model fitting and the prediction accuracy."
      ],
      "metadata": {
        "id": "pJml_PDnD8g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_min_value = np.min(np.abs(DEVICE_STATES))\n",
        "g_max_value = np.max(np.abs(DEVICE_STATES))\n",
        "    \n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "# will create matrix to simulate device-to-device variation by creating clipping the weights\n",
        "# will also simulate devices being stuck-on-open and stuck-on-close \n",
        "def initialize_variation_stuck_mat(shape):\n",
        "  # VARIATION\n",
        "  wp_max = np.ones(shape=shape) - np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # max. is smaller than 1\n",
        "  wp_min = np.zeros(shape=shape) + np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # min. is larger than 0\n",
        "\n",
        "  wm_max = np.ones(shape=shape) - np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # max. is smaller than 1\n",
        "  wm_min = np.zeros(shape=shape) + np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # min. is larger than 0\n",
        "\n",
        "  # STUCK\n",
        "  stuck_prob = [DEVICE_STUCK_OFF_PROB, 1 - DEVICE_STUCK_ON_PROB - DEVICE_STUCK_OFF_PROB, DEVICE_STUCK_ON_PROB]\n",
        "  # generate the stuck state according to the above probability - What does 0 mean?\n",
        "  w_p_stuck = np.random.choice([-1, 0, 1], size=shape, p=stuck_prob) \n",
        "  w_m_stuck = np.random.choice([-1, 0, 1], size=shape, p=stuck_prob)\n",
        "\n",
        "  # if device is stuck OFF\n",
        "  wp_max = wp_max + (w_p_stuck == -1) * (wp_min - wp_max)\n",
        "  wm_max = wm_max + (w_m_stuck == -1) * (wm_min - wm_max)\n",
        "\n",
        "  # if device is stuck ON\n",
        "  wp_min = wp_min + (w_p_stuck == 1) * (wp_max - wp_min)\n",
        "  wm_min = wm_min + (w_m_stuck == 1) * (wm_max - wm_min)\n",
        "\n",
        "\n",
        "  # PUTTING TOGETHER CLIPPING MATRIX\n",
        "  # numpy.clip(a, a_min, a_max) \n",
        "  # Clip (limit) the values in an array.\n",
        "  # Given an interval, values outside the interval are clipped to the interval edges\n",
        "  lower_lim = np.clip(wp_min - wm_max, -g_max_value, -g_min_value)\n",
        "  upper_lim = np.clip(wp_max - wm_min, g_min_value, g_max_value)\n",
        "\n",
        "  #print('Lower lim: ' + str(lower_lim))\n",
        "  #print('Upper lim: ' + str(upper_lim))\n",
        "\n",
        "  return [lower_lim, upper_lim]\n",
        "\n",
        "\n",
        "# weight update with a discrete number of states and (optional) add read noise\n",
        "def discrete_weight_update(value, read_noise_mean=0, read_noise_stddev=0):\n",
        "    if read_noise_stddev != 0:\n",
        "        value += np.random.normal(read_noise_mean, read_noise_stddev)\n",
        "    absolute_difference_function = lambda list_value : abs(list_value - value)\n",
        "    return min(DEVICE_STATES, key=absolute_difference_function)\n",
        "v_discrete_weight_update = np.vectorize(discrete_weight_update)\n",
        "\n",
        "\n",
        "\n",
        "# function puts together all the parts\n",
        "# 1. Device variation\n",
        "# 2. Stuck-on/off \n",
        "# 3. Discrete number of weight states\n",
        "# Input = software weights matrix, Output = hardware weights matrix\n",
        "def simulate_hardware_weight_update(weights_mat, var_stuck_mat):\n",
        "\n",
        "  # initialize variation and stuck matrix if not initialized\n",
        "  if type(var_stuck_mat) is not np.ndarray:\n",
        "      var_stuck_mat = initialize_variation_stuck_mat(weights_mat.shape)\n",
        "\n",
        "  # simulate weight variation and stuck on open/close\n",
        "  weights_mat = weights_mat.clip(var_stuck_mat[0], var_stuck_mat[1])\n",
        "\n",
        "  # simulate discrete states\n",
        "  weights_mat = v_discrete_weight_update(weights_mat, read_noise_mean = READ_NOISE_MEAN,\n",
        "                        read_noise_stddev = READ_NOISE_STDDEV)\n",
        "\n",
        "  return weights_mat"
      ],
      "metadata": {
        "id": "Z3T5K9-tEAt1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Old codes for CV**"
      ],
      "metadata": {
        "id": "YHjCoGUOGfpo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xDlMT8Cjtmj"
      },
      "source": [
        "# need to upgrade to tf 2.7.0\n",
        "\n",
        "def run_MLP_simulation(save_results_input = False,\n",
        "                       num_epochs_input=50, \n",
        "                       hardware_simulation_input=False, \n",
        "                       device_states_input=False,\n",
        "                       read_noise_mean_input=0,\n",
        "                       read_noise_stddev_input=0,\n",
        "                       device_variation_stddev_input=0,\n",
        "                       device_stuck_on_prob_input=0,\n",
        "                       device_stuck_off_prob_input=0):\n",
        "    \n",
        "    \n",
        "    tf.compat.v1.disable_v2_behavior() # disable v2 behavior\n",
        "    \n",
        "    ###################### USER DEFINED PARAMETERS FOR SIMULATION\n",
        "    # whether or not to save results\n",
        "    SAVE_RESULTS = save_results_input\n",
        "    \n",
        "    # number of epochs to test\n",
        "    NUM_EPOCHS = num_epochs_input\n",
        "\n",
        "    # flag which determines whether this is a hardware simulation or purely software\n",
        "    HARDWARE_SIMULATION = hardware_simulation_input\n",
        "\n",
        "    # parameter set by user that gives all the possible normalized weight states\n",
        "    # assumes (1) discrete number of states that are normalized \n",
        "    #         (2) states are set by two synaptic devices such that weight = weight_p - weight_m\n",
        "    #         (3) because of (2), weights can vary from [-1,1]\n",
        "    # user input = a 1D numpy array with values from [-1, 1]\n",
        "    DEVICE_STATES = device_states_input\n",
        "\n",
        "\n",
        "    # parameters for simulating read noise\n",
        "    # user input = read noise mean and standard dev assuming a normal noise function\n",
        "    READ_NOISE_MEAN = read_noise_mean_input\n",
        "    READ_NOISE_STDDEV = read_noise_stddev_input\n",
        "\n",
        "    # parameter for simulating device-to-device variation\n",
        "    # user input =  standard deviation of conductances\n",
        "    DEVICE_VARIATION_STDDEV = device_variation_stddev_input\n",
        "\n",
        "    # parameter for simulating devices that get stuck on Gmax or Gmin states from the start\n",
        "    # user input = probability for a device to get stuck\n",
        "    # What is G?\n",
        "    DEVICE_STUCK_ON_PROB = device_stuck_on_prob_input\n",
        "    DEVICE_STUCK_OFF_PROB = device_stuck_off_prob_input\n",
        "\n",
        "    \n",
        "    \n",
        "    ###################### SIM PARAMETERS\n",
        "    n_inputs = 28*28  # MNIST\n",
        "    n_hidden1 = 300 # neurons in 1st hidden layers\n",
        "    n_outputs = 10 # neurons in output layer\n",
        "    learning_rate = 0.1#0.01 # grad descent\n",
        "    initializer_stddev = 0.2 # standar deviation of initialized random weights\n",
        "    n_epochs = NUM_EPOCHS # number of epochs to test\n",
        "    batch_size = 50 # batch size before tuning weights in grad descent\n",
        "\n",
        "    # What are the possible values of DEVICE_STATES?\n",
        "    g_min_value = np.min(np.abs(DEVICE_STATES))\n",
        "    g_max_value = np.max(np.abs(DEVICE_STATES))\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ###################### FUNCTIONS\n",
        "\n",
        "    # to make this notebook's output stable across runs\n",
        "    def reset_graph(seed=42):\n",
        "        #tf.reset_default_graph() # Why reset_default_graph()? helpful in testing process?\n",
        "        tf.compat.v1.reset_default_graph\n",
        "        tf.compat.v1.set_random_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # will create matrix to simulate device-to-device variation by creating clipping the weights\n",
        "    # will also simulate devices being stuck-on-open and stuck-on-close \n",
        "    # What is wp, wm, and how do them relate to g?\n",
        "    def initialize_variation_stuck_mat(shape):\n",
        "\n",
        "        # VARIATION\n",
        "        wp_max = np.ones(shape=shape) - np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # max. is around 1\n",
        "        wp_min = np.zeros(shape=shape) + np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # min. is around 0\n",
        "\n",
        "        wm_max = np.ones(shape=shape) - np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # max. is around 1\n",
        "        wm_min = np.zeros(shape=shape) + np.abs(np.random.normal(0, DEVICE_VARIATION_STDDEV, shape)) # min. is around 0\n",
        "\n",
        "        # STUCK\n",
        "        stuck_prob = [DEVICE_STUCK_OFF_PROB, 1 - DEVICE_STUCK_ON_PROB - DEVICE_STUCK_OFF_PROB, DEVICE_STUCK_ON_PROB]\n",
        "        w_p_stuck = np.random.choice([-1, 0, 1], size=shape, p=stuck_prob)\n",
        "        w_m_stuck = np.random.choice([-1, 0, 1], size=shape, p=stuck_prob)\n",
        "\n",
        "        # if device is stuck OFF\n",
        "        wp_max = wp_max + (w_p_stuck == -1) * (wp_min - wp_max)\n",
        "        wm_max = wm_max + (w_m_stuck == -1) * (wm_min - wm_max)\n",
        "\n",
        "        # if device is stuck ON\n",
        "        wp_min = wp_min + (w_p_stuck == 1) * (wp_max - wp_min)\n",
        "        wm_min = wm_min + (w_m_stuck == 1) * (wm_max - wm_min)\n",
        "\n",
        "\n",
        "        # PUTTING TOGETHER CLIPPING MATRIX\n",
        "        lower_lim = np.clip(wp_min - wm_max, -g_max_value, -g_min_value)\n",
        "        upper_lim = np.clip(wp_max - wm_min, g_min_value, g_max_value)\n",
        "\n",
        "        print('Lower lim: ' + str(lower_lim))\n",
        "        print('Upper lim: ' + str(upper_lim))\n",
        "\n",
        "        return [lower_lim, upper_lim]\n",
        "\n",
        "\n",
        "    # weight update with a discrete number of states and (optional) add read noise\n",
        "    def discrete_weight_update(value, read_noise_mean=0, read_noise_stddev=0):\n",
        "        if read_noise_stddev != 0:\n",
        "            value += np.random.normal(read_noise_mean, read_noise_stddev)\n",
        "        absolute_difference_function = lambda list_value : abs(list_value - value)\n",
        "        return min(DEVICE_STATES, key=absolute_difference_function)\n",
        "    v_discrete_weight_update = np.vectorize(discrete_weight_update)\n",
        "\n",
        "\n",
        "\n",
        "    # function puts together all the parts\n",
        "    # 1. Device variation\n",
        "    # 2. Stuck-on/off \n",
        "    # 3. Discrete number of weight states\n",
        "    # Input = software weights matrix, Output = hardware weights matrix\n",
        "    def simulate_hardware_weight_update(weights_mat, var_stuck_mat):\n",
        "\n",
        "        # initialize variation and stuck matrix if not initialized\n",
        "        if type(var_stuck_mat) is not np.ndarray:\n",
        "            var_stuck_mat = initialize_variation_stuck_mat(weights_mat.shape)\n",
        "\n",
        "        # simulate weight variation and stuck on open/close\n",
        "        weights_mat = weights_mat.clip(var_stuck_mat[0], var_stuck_mat[1])\n",
        "\n",
        "        # simulate discrete states\n",
        "        weights_mat = v_discrete_weight_update(weights_mat, read_noise_mean = READ_NOISE_MEAN,\n",
        "                                                 read_noise_stddev = READ_NOISE_STDDEV)\n",
        "\n",
        "        return weights_mat\n",
        "    \n",
        "    \n",
        "    ###################### MLP SIM SETUP\n",
        "    # reset default tf graph before running sim\n",
        "    reset_graph()\n",
        "\n",
        "    # get data, format\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "    X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "    y_train = y_train.astype(np.int32)\n",
        "    y_test = y_test.astype(np.int32)\n",
        "    #X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "    #y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "\n",
        "    # define input and output placeholder variables\n",
        "    X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # input\n",
        "    y = tf.compat.v1.placeholder(tf.int32, shape=(None), name=\"y\") # output\n",
        "\n",
        "    # define NN layers\n",
        "    # tf.name_scope similar to namespace in C++, so that variable logits -> dnn/logits\n",
        "    with tf.name_scope(\"dnn\"): \n",
        "        initiliazer = tf.truncated_normal_initializer(stddev = initializer_stddev)\n",
        "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu, \n",
        "                                  kernel_initializer=initiliazer, bias_initializer=initiliazer)\n",
        "        logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\",\n",
        "                                 kernel_initializer=initiliazer, bias_initializer=initiliazer)\n",
        "        y_proba = tf.nn.softmax(logits)\n",
        "\n",
        "        #embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        #gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "        #dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # define loss\n",
        "    with tf.name_scope(\"loss\"): \n",
        "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "    # define training\n",
        "    with tf.name_scope(\"train\"):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        training_op = optimizer.minimize(loss)\n",
        "\n",
        "    # define recognition rate eval op\n",
        "    with tf.name_scope(\"eval\"):\n",
        "        correct = tf.nn.in_top_k(logits, y, 1)\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "    # define weight update ops\n",
        "    var_stuck_mat = [False, False, False, False]\n",
        "    weights = [0,0,0,0]\n",
        "    new_weights = [0,0,0,0]\n",
        "    weight_update_op = [0,0,0,0]\n",
        "    with tf.name_scope(\"weight_update\"):\n",
        "        weight_layers = [\"hidden1/kernel:0\", \"hidden1/bias:0\", \"outputs/kernel:0\", \"outputs/bias:0\"]\n",
        "        for i, name in enumerate(weight_layers):\n",
        "            weights[i] = [v for v in tf.trainable_variables() if v.name == name][0]\n",
        "            new_weights[i] = tf.placeholder(tf.float32, name=\"new_weights\"+name.replace(\"/\",\"-\").replace(\":\",\"-\"))\n",
        "            weight_update_op[i] = tf.assign(weights[i], new_weights[i])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ###################### MLP SIM RUN\n",
        "    epoch_ls = []\n",
        "    recognition_rate_ls = []\n",
        "    start_time = time.time()\n",
        "    start_datetime  = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    weights_before = False\n",
        "    weights_after = False\n",
        "\n",
        "    def shuffle_batch(X, y, batch_size):\n",
        "        rnd_idx = np.random.permutation(len(X)) # permute - 改变序列\n",
        "        n_batches = len(X) // batch_size \n",
        "        for batch_idx in np.array_split(rnd_idx, n_batches): # split rnd_idx into n_batches indexes\n",
        "            X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "            yield X_batch, y_batch\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        init.run()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "\n",
        "            if HARDWARE_SIMULATION:\n",
        "                ###### WEIGHT UPDATE  \n",
        "                # simulate hardware by updating weights\n",
        "                # includes discrete number of weight states\n",
        "\n",
        "                # the code below only updates hidden1/kernel:0 weights\n",
        "\n",
        "                # get weights\n",
        "                #weights_temp = [v for v in tf.trainable_variables() if v.name == \"hidden1/bias:0\"][0]\n",
        "                #weights_before = weights_temp.eval(session=sess)\n",
        "\n",
        "\n",
        "                # simulate hardware weight update\n",
        "                #new_weights_mat = simulate_hardware_weight_update(weights_mat, var_stuck_mat)\n",
        "\n",
        "                # update weights to hardware simulated weights\n",
        "\n",
        "                for i, weight in enumerate(weights):\n",
        "\n",
        "                    weight = weight.eval(session=sess)\n",
        "                    # !!! - where memtransistor works\n",
        "                    new_weights_mat = simulate_hardware_weight_update(weight, var_stuck_mat[i]) \n",
        "                    weight_update_op[i].eval(feed_dict={new_weights[i]: new_weights_mat})\n",
        "\n",
        "\n",
        "                ###### end of WEIGHT UPDATE\n",
        "\n",
        "\n",
        "\n",
        "            # test accuracy\n",
        "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            acc_valid = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "            print(epoch, \"Batch accuracy:\", acc_batch, \"Test accuracy:\", acc_valid)\n",
        "            \n",
        "            # save to list\n",
        "            epoch_ls.append(epoch)\n",
        "            recognition_rate_ls.append(acc_valid)\n",
        "\n",
        "            #weights_temp = [v for v in tf.trainable_variables() if v.name == \"hidden1/bias:0\"][0]\n",
        "            #weights_after = weights_temp.eval(session=sess)    \n",
        "\n",
        "\n",
        "        # save results\n",
        "        if SAVE_RESULTS:\n",
        "            save_path = \"./MLP_sim_results/MLP_sim_\" + start_datetime + \"/model_\" + start_datetime\n",
        "            save_path = saver.save(sess, save_path + \".ckpt\")\n",
        "    \n",
        "    # print out duration\n",
        "    print(\"--- %0.2f seconds ---\" % (time.time() - start_time))\n",
        "    \n",
        "    # save all results, including testing parameters and data\n",
        "    if SAVE_RESULTS:\n",
        "        save_path = \"./MLP_sim_results/MLP_sim_\" + start_datetime + \"/model_\" + start_datetime\n",
        "\n",
        "        data_df = pd.DataFrame({\"Epoch\": epoch_ls, \"Recognition Rate\": recognition_rate_ls})\n",
        "        data_df.to_csv(save_path + \"_data.csv\", index=False,)\n",
        "\n",
        "        with open(save_path + \"_meta.txt\", \"w\") as text_file:\n",
        "            print(\"---------- User input parameters -------------\", file=text_file)\n",
        "            print(\"Simulation start time: {}\".format(start_datetime), file=text_file)\n",
        "            print(\"Duration: {}\".format(time.time() - start_time), file=text_file)\n",
        "            print(\"Epochs: {}\".format(num_epochs_input), file=text_file)\n",
        "            print(\"Hardware simulation?: {}\".format(hardware_simulation_input), file=text_file)\n",
        "            print(\"Read noise - mean: {}\".format(read_noise_mean_input), file=text_file)\n",
        "            print(\"Read noise - standard deviation: {}\".format(read_noise_stddev_input), file=text_file)\n",
        "            print(\"Device variation - standard deviation: {}\".format(device_variation_stddev_input), file=text_file)\n",
        "            print(\"Device stuck on probability: {}\".format(device_stuck_on_prob_input), file=text_file)\n",
        "            print(\"Device stuck off probability: {}\".format(device_stuck_off_prob_input), file=text_file)\n",
        "            print(\"Device states used: {}\".format(device_states_input), file=text_file)\n",
        "\n",
        "            print(\"---------- Simulation parameters -------------\", file=text_file)\n",
        "            print(\"Number of inputs: {}\".format(n_inputs), file=text_file)\n",
        "            print(\"Layers: {}\".format(weight_layers), file=text_file)\n",
        "            print(\"Hidden1 # of neurons: {}\".format(n_hidden1), file=text_file)\n",
        "            print(\"Outputs # of neurons: {}\".format(n_outputs), file=text_file)\n",
        "            print(\"Learning rate: {}\".format(learning_rate), file=text_file)\n",
        "            print(\"Initializer standad dev: {}\".format(initializer_stddev), file=text_file)\n",
        "            print(\"Batch size: {}\".format(batch_size), file=text_file)\n",
        "\n",
        "\n",
        "# imports data from single column CSV file with possible current/conductance states\n",
        "# return numpy array of approximate states possible using this hardware\n",
        "# this import method is not generalized, but fine-tuned to Vinod's devices\n",
        "def import_data_from_csv(filename):\n",
        "    # import data\n",
        "    imported_device_states = np.genfromtxt(filename, delimiter=',')[1:]\n",
        "\n",
        "    # since data is in ~1 nA, assume maximum precision is ~1 pA\n",
        "    # this will make some states redundant\n",
        "    imported_device_states = np.unique(np.round(np.sort(imported_device_states), decimals=3))\n",
        "\n",
        "    # calculate device states possible\n",
        "    device_states = np.array([])\n",
        "    for i, value in enumerate(imported_device_states):\n",
        "        if i+1 > len(imported_device_states):\n",
        "            break\n",
        "        temp_ls = value - imported_device_states\n",
        "        device_states = np.append(device_states, temp_ls)\n",
        "\n",
        "\n",
        "    # normalize to -1 to 1\n",
        "    device_states = np.unique(np.sort(device_states))\n",
        "    device_states = device_states / np.abs(device_states).max()\n",
        "\n",
        "    # given the large number of states, we can assume some states are almost equivalent\n",
        "    # moreover, once the number of states is > 100, the discreteness doesnt matter\n",
        "    # for simplicity in the simulations, we will simply  round to 2 digits of the calculated states\n",
        "    device_states = np.round(device_states, decimals = 2)\n",
        "    device_states = np.unique(np.sort(device_states))\n",
        "    \n",
        "    return device_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "u8cD0K-M-pMw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "49a4e272-ebc4-47dc-bfc3-855f893f7309"
      },
      "source": [
        "# mock simulation using np.arange for device states\n",
        "for i in range(5):\n",
        "    run_MLP_simulation(num_epochs_input = 100, \n",
        "                       hardware_simulation_input = False, \n",
        "                       device_states_input = False,\n",
        "                       read_noise_mean_input = 0.0,\n",
        "                       read_noise_stddev_input = 0.0,\n",
        "                       device_variation_stddev_input = 0.0,\n",
        "                       device_stuck_on_prob_input = 0.0,\n",
        "                       device_stuck_off_prob_input = 0.0,\n",
        "                       save_results_input = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f1baf16b8945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0mdevice_stuck_on_prob_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mdevice_stuck_off_prob_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        save_results_input = True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-0e87d7bbeaf8>\u001b[0m in \u001b[0;36mrun_MLP_simulation\u001b[0;34m(save_results_input, num_epochs_input, hardware_simulation_input, device_states_input, read_noise_mean_input, read_noise_stddev_input, device_variation_stddev_input, device_stuck_on_prob_input, device_stuck_off_prob_input)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# define NN layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0minitiliazer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer_stddev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu, \n\u001b[1;32m    160\u001b[0m                                   kernel_initializer=initiliazer, bias_initializer=initiliazer)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'truncated_normal_initializer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rchKubR2-pMy",
        "outputId": "271df512-103c-4a55-c5e0-d8303c4f4e6f"
      },
      "source": [
        "# run hardware sim 4 times\n",
        "for i in range(4):\n",
        "    imported_device_states = import_data_from_csv(filename='learning_curve_vinod.csv')\n",
        "    run_MLP_simulation(num_epochs_input = 100, \n",
        "                       hardware_simulation_input = True, \n",
        "                       device_states_input = imported_device_states,\n",
        "                       read_noise_mean_input = 0.0,\n",
        "                       read_noise_stddev_input = 0.1,\n",
        "                       device_variation_stddev_input = 0.0,\n",
        "                       device_stuck_on_prob_input = 0.00,\n",
        "                       device_stuck_off_prob_input = 0.00,\n",
        "                       save_results_input = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 0.72 Test accuracy: 0.6556\n",
            "1 Batch accuracy: 0.6 Test accuracy: 0.5914\n",
            "2 Batch accuracy: 0.7 Test accuracy: 0.7075\n",
            "3 Batch accuracy: 0.5 Test accuracy: 0.5555\n",
            "4 Batch accuracy: 0.82 Test accuracy: 0.7896\n",
            "5 Batch accuracy: 0.76 Test accuracy: 0.7968\n",
            "6 Batch accuracy: 0.8 Test accuracy: 0.776\n",
            "7 Batch accuracy: 0.82 Test accuracy: 0.7581\n",
            "8 Batch accuracy: 0.9 Test accuracy: 0.8142\n",
            "9 Batch accuracy: 0.6 Test accuracy: 0.6915\n",
            "10 Batch accuracy: 0.78 Test accuracy: 0.8267\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.8334\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.8398\n",
            "13 Batch accuracy: 0.82 Test accuracy: 0.8183\n",
            "14 Batch accuracy: 0.8 Test accuracy: 0.843\n",
            "15 Batch accuracy: 0.92 Test accuracy: 0.846\n",
            "16 Batch accuracy: 0.78 Test accuracy: 0.826\n",
            "17 Batch accuracy: 0.82 Test accuracy: 0.8299\n",
            "18 Batch accuracy: 0.84 Test accuracy: 0.8476\n",
            "19 Batch accuracy: 0.86 Test accuracy: 0.8514\n",
            "20 Batch accuracy: 0.84 Test accuracy: 0.8119\n",
            "21 Batch accuracy: 0.88 Test accuracy: 0.829\n",
            "22 Batch accuracy: 0.78 Test accuracy: 0.8402\n",
            "23 Batch accuracy: 0.86 Test accuracy: 0.8478\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.8553\n",
            "25 Batch accuracy: 0.76 Test accuracy: 0.8315\n",
            "26 Batch accuracy: 0.88 Test accuracy: 0.8339\n",
            "27 Batch accuracy: 0.88 Test accuracy: 0.8776\n",
            "28 Batch accuracy: 0.84 Test accuracy: 0.8667\n",
            "29 Batch accuracy: 0.86 Test accuracy: 0.8583\n",
            "30 Batch accuracy: 0.86 Test accuracy: 0.874\n",
            "31 Batch accuracy: 0.92 Test accuracy: 0.8726\n",
            "32 Batch accuracy: 0.88 Test accuracy: 0.8804\n",
            "33 Batch accuracy: 0.88 Test accuracy: 0.8656\n",
            "34 Batch accuracy: 0.86 Test accuracy: 0.8395\n",
            "35 Batch accuracy: 0.88 Test accuracy: 0.865\n",
            "36 Batch accuracy: 0.88 Test accuracy: 0.8673\n",
            "37 Batch accuracy: 0.82 Test accuracy: 0.8524\n",
            "38 Batch accuracy: 0.8 Test accuracy: 0.8347\n",
            "39 Batch accuracy: 0.82 Test accuracy: 0.877\n",
            "40 Batch accuracy: 0.92 Test accuracy: 0.8657\n",
            "41 Batch accuracy: 0.82 Test accuracy: 0.881\n",
            "42 Batch accuracy: 0.84 Test accuracy: 0.8589\n",
            "43 Batch accuracy: 0.88 Test accuracy: 0.8753\n",
            "44 Batch accuracy: 0.84 Test accuracy: 0.8859\n",
            "45 Batch accuracy: 0.9 Test accuracy: 0.8712\n",
            "46 Batch accuracy: 0.86 Test accuracy: 0.869\n",
            "47 Batch accuracy: 0.82 Test accuracy: 0.8643\n",
            "48 Batch accuracy: 0.9 Test accuracy: 0.8693\n",
            "49 Batch accuracy: 0.92 Test accuracy: 0.8565\n",
            "50 Batch accuracy: 0.8 Test accuracy: 0.8866\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.8775\n",
            "52 Batch accuracy: 0.9 Test accuracy: 0.8685\n",
            "53 Batch accuracy: 0.74 Test accuracy: 0.8609\n",
            "54 Batch accuracy: 0.86 Test accuracy: 0.8633\n",
            "55 Batch accuracy: 0.92 Test accuracy: 0.8693\n",
            "56 Batch accuracy: 0.82 Test accuracy: 0.871\n",
            "57 Batch accuracy: 0.9 Test accuracy: 0.881\n",
            "58 Batch accuracy: 0.84 Test accuracy: 0.8849\n",
            "59 Batch accuracy: 0.9 Test accuracy: 0.8547\n",
            "60 Batch accuracy: 0.88 Test accuracy: 0.8763\n",
            "61 Batch accuracy: 0.88 Test accuracy: 0.8825\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.8704\n",
            "63 Batch accuracy: 0.88 Test accuracy: 0.8854\n",
            "64 Batch accuracy: 0.84 Test accuracy: 0.8656\n",
            "65 Batch accuracy: 0.9 Test accuracy: 0.8631\n",
            "66 Batch accuracy: 0.92 Test accuracy: 0.8702\n",
            "67 Batch accuracy: 0.92 Test accuracy: 0.8812\n",
            "68 Batch accuracy: 0.9 Test accuracy: 0.8789\n",
            "69 Batch accuracy: 0.9 Test accuracy: 0.8849\n",
            "70 Batch accuracy: 0.88 Test accuracy: 0.8887\n",
            "71 Batch accuracy: 0.88 Test accuracy: 0.8795\n",
            "72 Batch accuracy: 0.9 Test accuracy: 0.8821\n",
            "73 Batch accuracy: 0.94 Test accuracy: 0.8834\n",
            "74 Batch accuracy: 0.9 Test accuracy: 0.8834\n",
            "75 Batch accuracy: 0.94 Test accuracy: 0.884\n",
            "76 Batch accuracy: 0.92 Test accuracy: 0.8814\n",
            "77 Batch accuracy: 0.9 Test accuracy: 0.8798\n",
            "78 Batch accuracy: 0.92 Test accuracy: 0.8825\n",
            "79 Batch accuracy: 0.92 Test accuracy: 0.886\n",
            "80 Batch accuracy: 0.9 Test accuracy: 0.8741\n",
            "81 Batch accuracy: 0.8 Test accuracy: 0.8845\n",
            "82 Batch accuracy: 0.94 Test accuracy: 0.8958\n",
            "83 Batch accuracy: 0.88 Test accuracy: 0.9004\n",
            "84 Batch accuracy: 0.9 Test accuracy: 0.8872\n",
            "85 Batch accuracy: 0.82 Test accuracy: 0.8762\n",
            "86 Batch accuracy: 0.9 Test accuracy: 0.885\n",
            "87 Batch accuracy: 0.84 Test accuracy: 0.8654\n",
            "88 Batch accuracy: 0.94 Test accuracy: 0.8849\n",
            "89 Batch accuracy: 0.86 Test accuracy: 0.8693\n",
            "90 Batch accuracy: 0.94 Test accuracy: 0.8902\n",
            "91 Batch accuracy: 0.9 Test accuracy: 0.9053\n",
            "92 Batch accuracy: 0.84 Test accuracy: 0.8824\n",
            "93 Batch accuracy: 0.88 Test accuracy: 0.8821\n",
            "94 Batch accuracy: 0.86 Test accuracy: 0.875\n",
            "95 Batch accuracy: 0.98 Test accuracy: 0.8869\n",
            "96 Batch accuracy: 0.92 Test accuracy: 0.8769\n",
            "97 Batch accuracy: 0.82 Test accuracy: 0.8863\n",
            "98 Batch accuracy: 0.88 Test accuracy: 0.8889\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.8792\n",
            "--- 3070.01 seconds ---\n",
            "0 Batch accuracy: 0.72 Test accuracy: 0.6556\n",
            "1 Batch accuracy: 0.6 Test accuracy: 0.5914\n",
            "2 Batch accuracy: 0.7 Test accuracy: 0.7075\n",
            "3 Batch accuracy: 0.5 Test accuracy: 0.5555\n",
            "4 Batch accuracy: 0.82 Test accuracy: 0.7896\n",
            "5 Batch accuracy: 0.76 Test accuracy: 0.7968\n",
            "6 Batch accuracy: 0.8 Test accuracy: 0.776\n",
            "7 Batch accuracy: 0.82 Test accuracy: 0.7581\n",
            "8 Batch accuracy: 0.9 Test accuracy: 0.8142\n",
            "9 Batch accuracy: 0.6 Test accuracy: 0.6915\n",
            "10 Batch accuracy: 0.78 Test accuracy: 0.8267\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.8334\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.8398\n",
            "13 Batch accuracy: 0.82 Test accuracy: 0.8183\n",
            "14 Batch accuracy: 0.8 Test accuracy: 0.843\n",
            "15 Batch accuracy: 0.92 Test accuracy: 0.846\n",
            "16 Batch accuracy: 0.78 Test accuracy: 0.826\n",
            "17 Batch accuracy: 0.82 Test accuracy: 0.8299\n",
            "18 Batch accuracy: 0.84 Test accuracy: 0.8476\n",
            "19 Batch accuracy: 0.86 Test accuracy: 0.8514\n",
            "20 Batch accuracy: 0.84 Test accuracy: 0.8119\n",
            "21 Batch accuracy: 0.88 Test accuracy: 0.829\n",
            "22 Batch accuracy: 0.78 Test accuracy: 0.8402\n",
            "23 Batch accuracy: 0.86 Test accuracy: 0.8478\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.8553\n",
            "25 Batch accuracy: 0.76 Test accuracy: 0.8315\n",
            "26 Batch accuracy: 0.88 Test accuracy: 0.8339\n",
            "27 Batch accuracy: 0.88 Test accuracy: 0.8776\n",
            "28 Batch accuracy: 0.84 Test accuracy: 0.8667\n",
            "29 Batch accuracy: 0.86 Test accuracy: 0.8583\n",
            "30 Batch accuracy: 0.86 Test accuracy: 0.874\n",
            "31 Batch accuracy: 0.92 Test accuracy: 0.8726\n",
            "32 Batch accuracy: 0.88 Test accuracy: 0.8804\n",
            "33 Batch accuracy: 0.88 Test accuracy: 0.8656\n",
            "34 Batch accuracy: 0.86 Test accuracy: 0.8395\n",
            "35 Batch accuracy: 0.88 Test accuracy: 0.865\n",
            "36 Batch accuracy: 0.88 Test accuracy: 0.8673\n",
            "37 Batch accuracy: 0.82 Test accuracy: 0.8524\n",
            "38 Batch accuracy: 0.8 Test accuracy: 0.8347\n",
            "39 Batch accuracy: 0.82 Test accuracy: 0.877\n",
            "40 Batch accuracy: 0.92 Test accuracy: 0.8657\n",
            "41 Batch accuracy: 0.82 Test accuracy: 0.881\n",
            "42 Batch accuracy: 0.84 Test accuracy: 0.8589\n",
            "43 Batch accuracy: 0.88 Test accuracy: 0.8753\n",
            "44 Batch accuracy: 0.84 Test accuracy: 0.8859\n",
            "45 Batch accuracy: 0.9 Test accuracy: 0.8712\n",
            "46 Batch accuracy: 0.86 Test accuracy: 0.869\n",
            "47 Batch accuracy: 0.82 Test accuracy: 0.8643\n",
            "48 Batch accuracy: 0.9 Test accuracy: 0.8693\n",
            "49 Batch accuracy: 0.92 Test accuracy: 0.8565\n",
            "50 Batch accuracy: 0.8 Test accuracy: 0.8866\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.8775\n",
            "52 Batch accuracy: 0.9 Test accuracy: 0.8685\n",
            "53 Batch accuracy: 0.74 Test accuracy: 0.8609\n",
            "54 Batch accuracy: 0.86 Test accuracy: 0.8633\n",
            "55 Batch accuracy: 0.92 Test accuracy: 0.8693\n",
            "56 Batch accuracy: 0.82 Test accuracy: 0.871\n",
            "57 Batch accuracy: 0.9 Test accuracy: 0.881\n",
            "58 Batch accuracy: 0.84 Test accuracy: 0.8849\n",
            "59 Batch accuracy: 0.9 Test accuracy: 0.8547\n",
            "60 Batch accuracy: 0.88 Test accuracy: 0.8763\n",
            "61 Batch accuracy: 0.88 Test accuracy: 0.8825\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.8704\n",
            "63 Batch accuracy: 0.88 Test accuracy: 0.8854\n",
            "64 Batch accuracy: 0.84 Test accuracy: 0.8656\n",
            "65 Batch accuracy: 0.9 Test accuracy: 0.8631\n",
            "66 Batch accuracy: 0.92 Test accuracy: 0.8702\n",
            "67 Batch accuracy: 0.92 Test accuracy: 0.8812\n",
            "68 Batch accuracy: 0.9 Test accuracy: 0.8789\n",
            "69 Batch accuracy: 0.9 Test accuracy: 0.8849\n",
            "70 Batch accuracy: 0.88 Test accuracy: 0.8887\n",
            "71 Batch accuracy: 0.88 Test accuracy: 0.8795\n",
            "72 Batch accuracy: 0.9 Test accuracy: 0.8821\n",
            "73 Batch accuracy: 0.94 Test accuracy: 0.8834\n",
            "74 Batch accuracy: 0.9 Test accuracy: 0.8834\n",
            "75 Batch accuracy: 0.94 Test accuracy: 0.884\n",
            "76 Batch accuracy: 0.92 Test accuracy: 0.8814\n",
            "77 Batch accuracy: 0.9 Test accuracy: 0.8798\n",
            "78 Batch accuracy: 0.92 Test accuracy: 0.8825\n",
            "79 Batch accuracy: 0.92 Test accuracy: 0.886\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80 Batch accuracy: 0.9 Test accuracy: 0.8741\n",
            "81 Batch accuracy: 0.8 Test accuracy: 0.8845\n",
            "82 Batch accuracy: 0.94 Test accuracy: 0.8958\n",
            "83 Batch accuracy: 0.88 Test accuracy: 0.9004\n",
            "84 Batch accuracy: 0.9 Test accuracy: 0.8872\n",
            "85 Batch accuracy: 0.82 Test accuracy: 0.8762\n",
            "86 Batch accuracy: 0.9 Test accuracy: 0.885\n",
            "87 Batch accuracy: 0.84 Test accuracy: 0.8654\n",
            "88 Batch accuracy: 0.94 Test accuracy: 0.8849\n",
            "89 Batch accuracy: 0.86 Test accuracy: 0.8693\n",
            "90 Batch accuracy: 0.94 Test accuracy: 0.8902\n",
            "91 Batch accuracy: 0.9 Test accuracy: 0.9053\n",
            "92 Batch accuracy: 0.84 Test accuracy: 0.8824\n",
            "93 Batch accuracy: 0.88 Test accuracy: 0.8821\n",
            "94 Batch accuracy: 0.86 Test accuracy: 0.875\n",
            "95 Batch accuracy: 0.98 Test accuracy: 0.8869\n",
            "96 Batch accuracy: 0.92 Test accuracy: 0.8769\n",
            "97 Batch accuracy: 0.82 Test accuracy: 0.8863\n",
            "98 Batch accuracy: 0.88 Test accuracy: 0.8889\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.8792\n",
            "--- 3072.75 seconds ---\n",
            "0 Batch accuracy: 0.72 Test accuracy: 0.6556\n",
            "1 Batch accuracy: 0.6 Test accuracy: 0.5914\n",
            "2 Batch accuracy: 0.7 Test accuracy: 0.7075\n",
            "3 Batch accuracy: 0.5 Test accuracy: 0.5555\n",
            "4 Batch accuracy: 0.82 Test accuracy: 0.7896\n",
            "5 Batch accuracy: 0.76 Test accuracy: 0.7968\n",
            "6 Batch accuracy: 0.8 Test accuracy: 0.776\n",
            "7 Batch accuracy: 0.82 Test accuracy: 0.7581\n",
            "8 Batch accuracy: 0.9 Test accuracy: 0.8142\n",
            "9 Batch accuracy: 0.6 Test accuracy: 0.6915\n",
            "10 Batch accuracy: 0.78 Test accuracy: 0.8267\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.8334\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.8398\n",
            "13 Batch accuracy: 0.82 Test accuracy: 0.8183\n",
            "14 Batch accuracy: 0.8 Test accuracy: 0.843\n",
            "15 Batch accuracy: 0.92 Test accuracy: 0.846\n",
            "16 Batch accuracy: 0.78 Test accuracy: 0.826\n",
            "17 Batch accuracy: 0.82 Test accuracy: 0.8299\n",
            "18 Batch accuracy: 0.84 Test accuracy: 0.8476\n",
            "19 Batch accuracy: 0.86 Test accuracy: 0.8514\n",
            "20 Batch accuracy: 0.84 Test accuracy: 0.8119\n",
            "21 Batch accuracy: 0.88 Test accuracy: 0.829\n",
            "22 Batch accuracy: 0.78 Test accuracy: 0.8402\n",
            "23 Batch accuracy: 0.86 Test accuracy: 0.8478\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.8553\n",
            "25 Batch accuracy: 0.76 Test accuracy: 0.8315\n",
            "26 Batch accuracy: 0.88 Test accuracy: 0.8339\n",
            "27 Batch accuracy: 0.88 Test accuracy: 0.8776\n",
            "28 Batch accuracy: 0.84 Test accuracy: 0.8667\n",
            "29 Batch accuracy: 0.86 Test accuracy: 0.8583\n",
            "30 Batch accuracy: 0.86 Test accuracy: 0.874\n",
            "31 Batch accuracy: 0.92 Test accuracy: 0.8726\n",
            "32 Batch accuracy: 0.88 Test accuracy: 0.8804\n",
            "33 Batch accuracy: 0.88 Test accuracy: 0.8656\n",
            "34 Batch accuracy: 0.86 Test accuracy: 0.8395\n",
            "35 Batch accuracy: 0.88 Test accuracy: 0.865\n",
            "36 Batch accuracy: 0.88 Test accuracy: 0.8673\n",
            "37 Batch accuracy: 0.82 Test accuracy: 0.8524\n",
            "38 Batch accuracy: 0.8 Test accuracy: 0.8347\n",
            "39 Batch accuracy: 0.82 Test accuracy: 0.877\n",
            "40 Batch accuracy: 0.92 Test accuracy: 0.8657\n",
            "41 Batch accuracy: 0.82 Test accuracy: 0.881\n",
            "42 Batch accuracy: 0.84 Test accuracy: 0.8589\n",
            "43 Batch accuracy: 0.88 Test accuracy: 0.8753\n",
            "44 Batch accuracy: 0.84 Test accuracy: 0.8859\n",
            "45 Batch accuracy: 0.9 Test accuracy: 0.8712\n",
            "46 Batch accuracy: 0.86 Test accuracy: 0.869\n",
            "47 Batch accuracy: 0.82 Test accuracy: 0.8643\n",
            "48 Batch accuracy: 0.9 Test accuracy: 0.8693\n",
            "49 Batch accuracy: 0.92 Test accuracy: 0.8565\n",
            "50 Batch accuracy: 0.8 Test accuracy: 0.8866\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.8775\n",
            "52 Batch accuracy: 0.9 Test accuracy: 0.8685\n",
            "53 Batch accuracy: 0.74 Test accuracy: 0.8609\n",
            "54 Batch accuracy: 0.86 Test accuracy: 0.8633\n",
            "55 Batch accuracy: 0.92 Test accuracy: 0.8693\n",
            "56 Batch accuracy: 0.82 Test accuracy: 0.871\n",
            "57 Batch accuracy: 0.9 Test accuracy: 0.881\n",
            "58 Batch accuracy: 0.84 Test accuracy: 0.8849\n",
            "59 Batch accuracy: 0.9 Test accuracy: 0.8547\n",
            "60 Batch accuracy: 0.88 Test accuracy: 0.8763\n",
            "61 Batch accuracy: 0.88 Test accuracy: 0.8825\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.8704\n",
            "63 Batch accuracy: 0.88 Test accuracy: 0.8854\n",
            "64 Batch accuracy: 0.84 Test accuracy: 0.8656\n",
            "65 Batch accuracy: 0.9 Test accuracy: 0.8631\n",
            "66 Batch accuracy: 0.92 Test accuracy: 0.8702\n",
            "67 Batch accuracy: 0.92 Test accuracy: 0.8812\n",
            "68 Batch accuracy: 0.9 Test accuracy: 0.8789\n",
            "69 Batch accuracy: 0.9 Test accuracy: 0.8849\n",
            "70 Batch accuracy: 0.88 Test accuracy: 0.8887\n",
            "71 Batch accuracy: 0.88 Test accuracy: 0.8795\n",
            "72 Batch accuracy: 0.9 Test accuracy: 0.8821\n",
            "73 Batch accuracy: 0.94 Test accuracy: 0.8834\n",
            "74 Batch accuracy: 0.9 Test accuracy: 0.8834\n",
            "75 Batch accuracy: 0.94 Test accuracy: 0.884\n",
            "76 Batch accuracy: 0.92 Test accuracy: 0.8814\n",
            "77 Batch accuracy: 0.9 Test accuracy: 0.8798\n",
            "78 Batch accuracy: 0.92 Test accuracy: 0.8825\n",
            "79 Batch accuracy: 0.92 Test accuracy: 0.886\n",
            "80 Batch accuracy: 0.9 Test accuracy: 0.8741\n",
            "81 Batch accuracy: 0.8 Test accuracy: 0.8845\n",
            "82 Batch accuracy: 0.94 Test accuracy: 0.8958\n",
            "83 Batch accuracy: 0.88 Test accuracy: 0.9004\n",
            "84 Batch accuracy: 0.9 Test accuracy: 0.8872\n",
            "85 Batch accuracy: 0.82 Test accuracy: 0.8762\n",
            "86 Batch accuracy: 0.9 Test accuracy: 0.885\n",
            "87 Batch accuracy: 0.84 Test accuracy: 0.8654\n",
            "88 Batch accuracy: 0.94 Test accuracy: 0.8849\n",
            "89 Batch accuracy: 0.86 Test accuracy: 0.8693\n",
            "90 Batch accuracy: 0.94 Test accuracy: 0.8902\n",
            "91 Batch accuracy: 0.9 Test accuracy: 0.9053\n",
            "92 Batch accuracy: 0.84 Test accuracy: 0.8824\n",
            "93 Batch accuracy: 0.88 Test accuracy: 0.8821\n",
            "94 Batch accuracy: 0.86 Test accuracy: 0.875\n",
            "95 Batch accuracy: 0.98 Test accuracy: 0.8869\n",
            "96 Batch accuracy: 0.92 Test accuracy: 0.8769\n",
            "97 Batch accuracy: 0.82 Test accuracy: 0.8863\n",
            "98 Batch accuracy: 0.88 Test accuracy: 0.8889\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.8792\n",
            "--- 3070.62 seconds ---\n",
            "0 Batch accuracy: 0.72 Test accuracy: 0.6556\n",
            "1 Batch accuracy: 0.6 Test accuracy: 0.5914\n",
            "2 Batch accuracy: 0.7 Test accuracy: 0.7075\n",
            "3 Batch accuracy: 0.5 Test accuracy: 0.5555\n",
            "4 Batch accuracy: 0.82 Test accuracy: 0.7896\n",
            "5 Batch accuracy: 0.76 Test accuracy: 0.7968\n",
            "6 Batch accuracy: 0.8 Test accuracy: 0.776\n",
            "7 Batch accuracy: 0.82 Test accuracy: 0.7581\n",
            "8 Batch accuracy: 0.9 Test accuracy: 0.8142\n",
            "9 Batch accuracy: 0.6 Test accuracy: 0.6915\n",
            "10 Batch accuracy: 0.78 Test accuracy: 0.8267\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.8334\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.8398\n",
            "13 Batch accuracy: 0.82 Test accuracy: 0.8183\n",
            "14 Batch accuracy: 0.8 Test accuracy: 0.843\n",
            "15 Batch accuracy: 0.92 Test accuracy: 0.846\n",
            "16 Batch accuracy: 0.78 Test accuracy: 0.826\n",
            "17 Batch accuracy: 0.82 Test accuracy: 0.8299\n",
            "18 Batch accuracy: 0.84 Test accuracy: 0.8476\n",
            "19 Batch accuracy: 0.86 Test accuracy: 0.8514\n",
            "20 Batch accuracy: 0.84 Test accuracy: 0.8119\n",
            "21 Batch accuracy: 0.88 Test accuracy: 0.829\n",
            "22 Batch accuracy: 0.78 Test accuracy: 0.8402\n",
            "23 Batch accuracy: 0.86 Test accuracy: 0.8478\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.8553\n",
            "25 Batch accuracy: 0.76 Test accuracy: 0.8315\n",
            "26 Batch accuracy: 0.88 Test accuracy: 0.8339\n",
            "27 Batch accuracy: 0.88 Test accuracy: 0.8776\n",
            "28 Batch accuracy: 0.84 Test accuracy: 0.8667\n",
            "29 Batch accuracy: 0.86 Test accuracy: 0.8583\n",
            "30 Batch accuracy: 0.86 Test accuracy: 0.874\n",
            "31 Batch accuracy: 0.92 Test accuracy: 0.8726\n",
            "32 Batch accuracy: 0.88 Test accuracy: 0.8804\n",
            "33 Batch accuracy: 0.88 Test accuracy: 0.8656\n",
            "34 Batch accuracy: 0.86 Test accuracy: 0.8395\n",
            "35 Batch accuracy: 0.88 Test accuracy: 0.865\n",
            "36 Batch accuracy: 0.88 Test accuracy: 0.8673\n",
            "37 Batch accuracy: 0.82 Test accuracy: 0.8524\n",
            "38 Batch accuracy: 0.8 Test accuracy: 0.8347\n",
            "39 Batch accuracy: 0.82 Test accuracy: 0.877\n",
            "40 Batch accuracy: 0.92 Test accuracy: 0.8657\n",
            "41 Batch accuracy: 0.82 Test accuracy: 0.881\n",
            "42 Batch accuracy: 0.84 Test accuracy: 0.8589\n",
            "43 Batch accuracy: 0.88 Test accuracy: 0.8753\n",
            "44 Batch accuracy: 0.84 Test accuracy: 0.8859\n",
            "45 Batch accuracy: 0.9 Test accuracy: 0.8712\n",
            "46 Batch accuracy: 0.86 Test accuracy: 0.869\n",
            "47 Batch accuracy: 0.82 Test accuracy: 0.8643\n",
            "48 Batch accuracy: 0.9 Test accuracy: 0.8693\n",
            "49 Batch accuracy: 0.92 Test accuracy: 0.8565\n",
            "50 Batch accuracy: 0.8 Test accuracy: 0.8866\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.8775\n",
            "52 Batch accuracy: 0.9 Test accuracy: 0.8685\n",
            "53 Batch accuracy: 0.74 Test accuracy: 0.8609\n",
            "54 Batch accuracy: 0.86 Test accuracy: 0.8633\n",
            "55 Batch accuracy: 0.92 Test accuracy: 0.8693\n",
            "56 Batch accuracy: 0.82 Test accuracy: 0.871\n",
            "57 Batch accuracy: 0.9 Test accuracy: 0.881\n",
            "58 Batch accuracy: 0.84 Test accuracy: 0.8849\n",
            "59 Batch accuracy: 0.9 Test accuracy: 0.8547\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60 Batch accuracy: 0.88 Test accuracy: 0.8763\n",
            "61 Batch accuracy: 0.88 Test accuracy: 0.8825\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.8704\n",
            "63 Batch accuracy: 0.88 Test accuracy: 0.8854\n",
            "64 Batch accuracy: 0.84 Test accuracy: 0.8656\n",
            "65 Batch accuracy: 0.9 Test accuracy: 0.8631\n",
            "66 Batch accuracy: 0.92 Test accuracy: 0.8702\n",
            "67 Batch accuracy: 0.92 Test accuracy: 0.8812\n",
            "68 Batch accuracy: 0.9 Test accuracy: 0.8789\n",
            "69 Batch accuracy: 0.9 Test accuracy: 0.8849\n",
            "70 Batch accuracy: 0.88 Test accuracy: 0.8887\n",
            "71 Batch accuracy: 0.88 Test accuracy: 0.8795\n",
            "72 Batch accuracy: 0.9 Test accuracy: 0.8821\n",
            "73 Batch accuracy: 0.94 Test accuracy: 0.8834\n",
            "74 Batch accuracy: 0.9 Test accuracy: 0.8834\n",
            "75 Batch accuracy: 0.94 Test accuracy: 0.884\n",
            "76 Batch accuracy: 0.92 Test accuracy: 0.8814\n",
            "77 Batch accuracy: 0.9 Test accuracy: 0.8798\n",
            "78 Batch accuracy: 0.92 Test accuracy: 0.8825\n",
            "79 Batch accuracy: 0.92 Test accuracy: 0.886\n",
            "80 Batch accuracy: 0.9 Test accuracy: 0.8741\n",
            "81 Batch accuracy: 0.8 Test accuracy: 0.8845\n",
            "82 Batch accuracy: 0.94 Test accuracy: 0.8958\n",
            "83 Batch accuracy: 0.88 Test accuracy: 0.9004\n",
            "84 Batch accuracy: 0.9 Test accuracy: 0.8872\n",
            "85 Batch accuracy: 0.82 Test accuracy: 0.8762\n",
            "86 Batch accuracy: 0.9 Test accuracy: 0.885\n",
            "87 Batch accuracy: 0.84 Test accuracy: 0.8654\n",
            "88 Batch accuracy: 0.94 Test accuracy: 0.8849\n",
            "89 Batch accuracy: 0.86 Test accuracy: 0.8693\n",
            "90 Batch accuracy: 0.94 Test accuracy: 0.8902\n",
            "91 Batch accuracy: 0.9 Test accuracy: 0.9053\n",
            "92 Batch accuracy: 0.84 Test accuracy: 0.8824\n",
            "93 Batch accuracy: 0.88 Test accuracy: 0.8821\n",
            "94 Batch accuracy: 0.86 Test accuracy: 0.875\n",
            "95 Batch accuracy: 0.98 Test accuracy: 0.8869\n",
            "96 Batch accuracy: 0.92 Test accuracy: 0.8769\n",
            "97 Batch accuracy: 0.82 Test accuracy: 0.8863\n",
            "98 Batch accuracy: 0.88 Test accuracy: 0.8889\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.8792\n",
            "--- 3052.23 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U20Mg0qp-pMz",
        "outputId": "d6293b16-3922-4486-c341-8b7d8d5441bd"
      },
      "source": [
        "# run hardware sim 4 times\n",
        "for i in range(4):\n",
        "    imported_device_states = import_data_from_csv(filename='learning_curve_vinod.csv')\n",
        "    run_MLP_simulation(num_epochs_input = 100, \n",
        "                       hardware_simulation_input = True, \n",
        "                       device_states_input = imported_device_states,\n",
        "                       read_noise_mean_input = 0.0,\n",
        "                       read_noise_stddev_input = 0.1,\n",
        "                       device_variation_stddev_input = 0.0,\n",
        "                       device_stuck_on_prob_input = 0.00,\n",
        "                       device_stuck_off_prob_input = 0.00,\n",
        "                       save_results_input = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 0.84 Test accuracy: 0.761\n",
            "1 Batch accuracy: 0.86 Test accuracy: 0.8079\n",
            "2 Batch accuracy: 0.88 Test accuracy: 0.8493\n",
            "3 Batch accuracy: 0.84 Test accuracy: 0.823\n",
            "4 Batch accuracy: 0.92 Test accuracy: 0.9038\n",
            "5 Batch accuracy: 0.86 Test accuracy: 0.9098\n",
            "6 Batch accuracy: 0.96 Test accuracy: 0.9185\n",
            "7 Batch accuracy: 0.92 Test accuracy: 0.8956\n",
            "8 Batch accuracy: 0.94 Test accuracy: 0.9088\n",
            "9 Batch accuracy: 0.86 Test accuracy: 0.8886\n",
            "10 Batch accuracy: 0.92 Test accuracy: 0.9233\n",
            "11 Batch accuracy: 0.94 Test accuracy: 0.919\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.917\n",
            "13 Batch accuracy: 0.96 Test accuracy: 0.9239\n",
            "14 Batch accuracy: 0.9 Test accuracy: 0.9272\n",
            "15 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "16 Batch accuracy: 0.94 Test accuracy: 0.9288\n",
            "17 Batch accuracy: 0.96 Test accuracy: 0.9197\n",
            "18 Batch accuracy: 0.9 Test accuracy: 0.9308\n",
            "19 Batch accuracy: 0.88 Test accuracy: 0.9271\n",
            "20 Batch accuracy: 0.96 Test accuracy: 0.9229\n",
            "21 Batch accuracy: 0.92 Test accuracy: 0.9356\n",
            "22 Batch accuracy: 0.94 Test accuracy: 0.9279\n",
            "23 Batch accuracy: 0.96 Test accuracy: 0.9297\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.9353\n",
            "25 Batch accuracy: 0.96 Test accuracy: 0.9387\n",
            "26 Batch accuracy: 1.0 Test accuracy: 0.9218\n",
            "27 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "28 Batch accuracy: 0.94 Test accuracy: 0.9349\n",
            "29 Batch accuracy: 0.96 Test accuracy: 0.9364\n",
            "30 Batch accuracy: 0.98 Test accuracy: 0.9324\n",
            "31 Batch accuracy: 0.94 Test accuracy: 0.935\n",
            "32 Batch accuracy: 1.0 Test accuracy: 0.9399\n",
            "33 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "34 Batch accuracy: 0.98 Test accuracy: 0.935\n",
            "35 Batch accuracy: 0.94 Test accuracy: 0.9466\n",
            "36 Batch accuracy: 0.9 Test accuracy: 0.944\n",
            "37 Batch accuracy: 0.94 Test accuracy: 0.9444\n",
            "38 Batch accuracy: 0.96 Test accuracy: 0.9407\n",
            "39 Batch accuracy: 0.94 Test accuracy: 0.9463\n",
            "40 Batch accuracy: 0.98 Test accuracy: 0.9426\n",
            "41 Batch accuracy: 0.94 Test accuracy: 0.9435\n",
            "42 Batch accuracy: 1.0 Test accuracy: 0.9382\n",
            "43 Batch accuracy: 0.96 Test accuracy: 0.9384\n",
            "44 Batch accuracy: 0.98 Test accuracy: 0.9435\n",
            "45 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "46 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "47 Batch accuracy: 0.94 Test accuracy: 0.9443\n",
            "48 Batch accuracy: 1.0 Test accuracy: 0.9482\n",
            "49 Batch accuracy: 0.96 Test accuracy: 0.9454\n",
            "50 Batch accuracy: 0.98 Test accuracy: 0.9465\n",
            "51 Batch accuracy: 0.98 Test accuracy: 0.946\n",
            "52 Batch accuracy: 0.94 Test accuracy: 0.9357\n",
            "53 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "54 Batch accuracy: 0.98 Test accuracy: 0.9372\n",
            "55 Batch accuracy: 0.96 Test accuracy: 0.9424\n",
            "56 Batch accuracy: 1.0 Test accuracy: 0.9368\n",
            "57 Batch accuracy: 0.94 Test accuracy: 0.9358\n",
            "58 Batch accuracy: 1.0 Test accuracy: 0.9432\n",
            "59 Batch accuracy: 0.96 Test accuracy: 0.9416\n",
            "60 Batch accuracy: 0.9 Test accuracy: 0.9444\n",
            "61 Batch accuracy: 0.98 Test accuracy: 0.9438\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.9366\n",
            "63 Batch accuracy: 0.96 Test accuracy: 0.9392\n",
            "64 Batch accuracy: 0.98 Test accuracy: 0.9374\n",
            "65 Batch accuracy: 0.98 Test accuracy: 0.947\n",
            "66 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
            "67 Batch accuracy: 0.98 Test accuracy: 0.9416\n",
            "68 Batch accuracy: 0.94 Test accuracy: 0.9412\n",
            "69 Batch accuracy: 0.92 Test accuracy: 0.9447\n",
            "70 Batch accuracy: 0.94 Test accuracy: 0.9425\n",
            "71 Batch accuracy: 0.96 Test accuracy: 0.9431\n",
            "72 Batch accuracy: 0.96 Test accuracy: 0.9421\n",
            "73 Batch accuracy: 0.96 Test accuracy: 0.9363\n",
            "74 Batch accuracy: 0.96 Test accuracy: 0.9467\n",
            "75 Batch accuracy: 0.98 Test accuracy: 0.9457\n",
            "76 Batch accuracy: 0.94 Test accuracy: 0.9445\n",
            "77 Batch accuracy: 0.98 Test accuracy: 0.9473\n",
            "78 Batch accuracy: 0.96 Test accuracy: 0.9428\n",
            "79 Batch accuracy: 0.98 Test accuracy: 0.9432\n",
            "80 Batch accuracy: 0.96 Test accuracy: 0.9453\n",
            "81 Batch accuracy: 0.98 Test accuracy: 0.9485\n",
            "82 Batch accuracy: 0.96 Test accuracy: 0.9482\n",
            "83 Batch accuracy: 0.96 Test accuracy: 0.9417\n",
            "84 Batch accuracy: 0.96 Test accuracy: 0.9472\n",
            "85 Batch accuracy: 0.94 Test accuracy: 0.9449\n",
            "86 Batch accuracy: 0.98 Test accuracy: 0.9481\n",
            "87 Batch accuracy: 0.98 Test accuracy: 0.9496\n",
            "88 Batch accuracy: 0.96 Test accuracy: 0.9505\n",
            "89 Batch accuracy: 0.94 Test accuracy: 0.9453\n",
            "90 Batch accuracy: 0.98 Test accuracy: 0.9494\n",
            "91 Batch accuracy: 1.0 Test accuracy: 0.952\n",
            "92 Batch accuracy: 0.96 Test accuracy: 0.9466\n",
            "93 Batch accuracy: 0.94 Test accuracy: 0.9496\n",
            "94 Batch accuracy: 0.94 Test accuracy: 0.944\n",
            "95 Batch accuracy: 1.0 Test accuracy: 0.9475\n",
            "96 Batch accuracy: 0.96 Test accuracy: 0.9516\n",
            "97 Batch accuracy: 0.96 Test accuracy: 0.9425\n",
            "98 Batch accuracy: 0.96 Test accuracy: 0.9457\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.9451\n",
            "--- 3061.78 seconds ---\n",
            "0 Batch accuracy: 0.84 Test accuracy: 0.761\n",
            "1 Batch accuracy: 0.86 Test accuracy: 0.8079\n",
            "2 Batch accuracy: 0.88 Test accuracy: 0.8493\n",
            "3 Batch accuracy: 0.84 Test accuracy: 0.823\n",
            "4 Batch accuracy: 0.92 Test accuracy: 0.9038\n",
            "5 Batch accuracy: 0.86 Test accuracy: 0.9098\n",
            "6 Batch accuracy: 0.96 Test accuracy: 0.9185\n",
            "7 Batch accuracy: 0.92 Test accuracy: 0.8956\n",
            "8 Batch accuracy: 0.94 Test accuracy: 0.9088\n",
            "9 Batch accuracy: 0.86 Test accuracy: 0.8886\n",
            "10 Batch accuracy: 0.92 Test accuracy: 0.9233\n",
            "11 Batch accuracy: 0.94 Test accuracy: 0.919\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.917\n",
            "13 Batch accuracy: 0.96 Test accuracy: 0.9239\n",
            "14 Batch accuracy: 0.9 Test accuracy: 0.9272\n",
            "15 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "16 Batch accuracy: 0.94 Test accuracy: 0.9288\n",
            "17 Batch accuracy: 0.96 Test accuracy: 0.9197\n",
            "18 Batch accuracy: 0.9 Test accuracy: 0.9308\n",
            "19 Batch accuracy: 0.88 Test accuracy: 0.9271\n",
            "20 Batch accuracy: 0.96 Test accuracy: 0.9229\n",
            "21 Batch accuracy: 0.92 Test accuracy: 0.9356\n",
            "22 Batch accuracy: 0.94 Test accuracy: 0.9279\n",
            "23 Batch accuracy: 0.96 Test accuracy: 0.9297\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.9353\n",
            "25 Batch accuracy: 0.96 Test accuracy: 0.9387\n",
            "26 Batch accuracy: 1.0 Test accuracy: 0.9218\n",
            "27 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "28 Batch accuracy: 0.94 Test accuracy: 0.9349\n",
            "29 Batch accuracy: 0.96 Test accuracy: 0.9364\n",
            "30 Batch accuracy: 0.98 Test accuracy: 0.9324\n",
            "31 Batch accuracy: 0.94 Test accuracy: 0.935\n",
            "32 Batch accuracy: 1.0 Test accuracy: 0.9399\n",
            "33 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "34 Batch accuracy: 0.98 Test accuracy: 0.935\n",
            "35 Batch accuracy: 0.94 Test accuracy: 0.9466\n",
            "36 Batch accuracy: 0.9 Test accuracy: 0.944\n",
            "37 Batch accuracy: 0.94 Test accuracy: 0.9444\n",
            "38 Batch accuracy: 0.96 Test accuracy: 0.9407\n",
            "39 Batch accuracy: 0.94 Test accuracy: 0.9463\n",
            "40 Batch accuracy: 0.98 Test accuracy: 0.9426\n",
            "41 Batch accuracy: 0.94 Test accuracy: 0.9435\n",
            "42 Batch accuracy: 1.0 Test accuracy: 0.9382\n",
            "43 Batch accuracy: 0.96 Test accuracy: 0.9384\n",
            "44 Batch accuracy: 0.98 Test accuracy: 0.9435\n",
            "45 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "46 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "47 Batch accuracy: 0.94 Test accuracy: 0.9443\n",
            "48 Batch accuracy: 1.0 Test accuracy: 0.9482\n",
            "49 Batch accuracy: 0.96 Test accuracy: 0.9454\n",
            "50 Batch accuracy: 0.98 Test accuracy: 0.9465\n",
            "51 Batch accuracy: 0.98 Test accuracy: 0.946\n",
            "52 Batch accuracy: 0.94 Test accuracy: 0.9357\n",
            "53 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "54 Batch accuracy: 0.98 Test accuracy: 0.9372\n",
            "55 Batch accuracy: 0.96 Test accuracy: 0.9424\n",
            "56 Batch accuracy: 1.0 Test accuracy: 0.9368\n",
            "57 Batch accuracy: 0.94 Test accuracy: 0.9358\n",
            "58 Batch accuracy: 1.0 Test accuracy: 0.9432\n",
            "59 Batch accuracy: 0.96 Test accuracy: 0.9416\n",
            "60 Batch accuracy: 0.9 Test accuracy: 0.9444\n",
            "61 Batch accuracy: 0.98 Test accuracy: 0.9438\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.9366\n",
            "63 Batch accuracy: 0.96 Test accuracy: 0.9392\n",
            "64 Batch accuracy: 0.98 Test accuracy: 0.9374\n",
            "65 Batch accuracy: 0.98 Test accuracy: 0.947\n",
            "66 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
            "67 Batch accuracy: 0.98 Test accuracy: 0.9416\n",
            "68 Batch accuracy: 0.94 Test accuracy: 0.9412\n",
            "69 Batch accuracy: 0.92 Test accuracy: 0.9447\n",
            "70 Batch accuracy: 0.94 Test accuracy: 0.9425\n",
            "71 Batch accuracy: 0.96 Test accuracy: 0.9431\n",
            "72 Batch accuracy: 0.96 Test accuracy: 0.9421\n",
            "73 Batch accuracy: 0.96 Test accuracy: 0.9363\n",
            "74 Batch accuracy: 0.96 Test accuracy: 0.9467\n",
            "75 Batch accuracy: 0.98 Test accuracy: 0.9457\n",
            "76 Batch accuracy: 0.94 Test accuracy: 0.9445\n",
            "77 Batch accuracy: 0.98 Test accuracy: 0.9473\n",
            "78 Batch accuracy: 0.96 Test accuracy: 0.9428\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79 Batch accuracy: 0.98 Test accuracy: 0.9432\n",
            "80 Batch accuracy: 0.96 Test accuracy: 0.9453\n",
            "81 Batch accuracy: 0.98 Test accuracy: 0.9485\n",
            "82 Batch accuracy: 0.96 Test accuracy: 0.9482\n",
            "83 Batch accuracy: 0.96 Test accuracy: 0.9417\n",
            "84 Batch accuracy: 0.96 Test accuracy: 0.9472\n",
            "85 Batch accuracy: 0.94 Test accuracy: 0.9449\n",
            "86 Batch accuracy: 0.98 Test accuracy: 0.9481\n",
            "87 Batch accuracy: 0.98 Test accuracy: 0.9496\n",
            "88 Batch accuracy: 0.96 Test accuracy: 0.9505\n",
            "89 Batch accuracy: 0.94 Test accuracy: 0.9453\n",
            "90 Batch accuracy: 0.98 Test accuracy: 0.9494\n",
            "91 Batch accuracy: 1.0 Test accuracy: 0.952\n",
            "92 Batch accuracy: 0.96 Test accuracy: 0.9466\n",
            "93 Batch accuracy: 0.94 Test accuracy: 0.9496\n",
            "94 Batch accuracy: 0.94 Test accuracy: 0.944\n",
            "95 Batch accuracy: 1.0 Test accuracy: 0.9475\n",
            "96 Batch accuracy: 0.96 Test accuracy: 0.9516\n",
            "97 Batch accuracy: 0.96 Test accuracy: 0.9425\n",
            "98 Batch accuracy: 0.96 Test accuracy: 0.9457\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.9451\n",
            "--- 3079.70 seconds ---\n",
            "0 Batch accuracy: 0.84 Test accuracy: 0.761\n",
            "1 Batch accuracy: 0.86 Test accuracy: 0.8079\n",
            "2 Batch accuracy: 0.88 Test accuracy: 0.8493\n",
            "3 Batch accuracy: 0.84 Test accuracy: 0.823\n",
            "4 Batch accuracy: 0.92 Test accuracy: 0.9038\n",
            "5 Batch accuracy: 0.86 Test accuracy: 0.9098\n",
            "6 Batch accuracy: 0.96 Test accuracy: 0.9185\n",
            "7 Batch accuracy: 0.92 Test accuracy: 0.8956\n",
            "8 Batch accuracy: 0.94 Test accuracy: 0.9088\n",
            "9 Batch accuracy: 0.86 Test accuracy: 0.8886\n",
            "10 Batch accuracy: 0.92 Test accuracy: 0.9233\n",
            "11 Batch accuracy: 0.94 Test accuracy: 0.919\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.917\n",
            "13 Batch accuracy: 0.96 Test accuracy: 0.9239\n",
            "14 Batch accuracy: 0.9 Test accuracy: 0.9272\n",
            "15 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "16 Batch accuracy: 0.94 Test accuracy: 0.9288\n",
            "17 Batch accuracy: 0.96 Test accuracy: 0.9197\n",
            "18 Batch accuracy: 0.9 Test accuracy: 0.9308\n",
            "19 Batch accuracy: 0.88 Test accuracy: 0.9271\n",
            "20 Batch accuracy: 0.96 Test accuracy: 0.9229\n",
            "21 Batch accuracy: 0.92 Test accuracy: 0.9356\n",
            "22 Batch accuracy: 0.94 Test accuracy: 0.9279\n",
            "23 Batch accuracy: 0.96 Test accuracy: 0.9297\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.9353\n",
            "25 Batch accuracy: 0.96 Test accuracy: 0.9387\n",
            "26 Batch accuracy: 1.0 Test accuracy: 0.9218\n",
            "27 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "28 Batch accuracy: 0.94 Test accuracy: 0.9349\n",
            "29 Batch accuracy: 0.96 Test accuracy: 0.9364\n",
            "30 Batch accuracy: 0.98 Test accuracy: 0.9324\n",
            "31 Batch accuracy: 0.94 Test accuracy: 0.935\n",
            "32 Batch accuracy: 1.0 Test accuracy: 0.9399\n",
            "33 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "34 Batch accuracy: 0.98 Test accuracy: 0.935\n",
            "35 Batch accuracy: 0.94 Test accuracy: 0.9466\n",
            "36 Batch accuracy: 0.9 Test accuracy: 0.944\n",
            "37 Batch accuracy: 0.94 Test accuracy: 0.9444\n",
            "38 Batch accuracy: 0.96 Test accuracy: 0.9407\n",
            "39 Batch accuracy: 0.94 Test accuracy: 0.9463\n",
            "40 Batch accuracy: 0.98 Test accuracy: 0.9426\n",
            "41 Batch accuracy: 0.94 Test accuracy: 0.9435\n",
            "42 Batch accuracy: 1.0 Test accuracy: 0.9382\n",
            "43 Batch accuracy: 0.96 Test accuracy: 0.9384\n",
            "44 Batch accuracy: 0.98 Test accuracy: 0.9435\n",
            "45 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "46 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "47 Batch accuracy: 0.94 Test accuracy: 0.9443\n",
            "48 Batch accuracy: 1.0 Test accuracy: 0.9482\n",
            "49 Batch accuracy: 0.96 Test accuracy: 0.9454\n",
            "50 Batch accuracy: 0.98 Test accuracy: 0.9465\n",
            "51 Batch accuracy: 0.98 Test accuracy: 0.946\n",
            "52 Batch accuracy: 0.94 Test accuracy: 0.9357\n",
            "53 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "54 Batch accuracy: 0.98 Test accuracy: 0.9372\n",
            "55 Batch accuracy: 0.96 Test accuracy: 0.9424\n",
            "56 Batch accuracy: 1.0 Test accuracy: 0.9368\n",
            "57 Batch accuracy: 0.94 Test accuracy: 0.9358\n",
            "58 Batch accuracy: 1.0 Test accuracy: 0.9432\n",
            "59 Batch accuracy: 0.96 Test accuracy: 0.9416\n",
            "60 Batch accuracy: 0.9 Test accuracy: 0.9444\n",
            "61 Batch accuracy: 0.98 Test accuracy: 0.9438\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.9366\n",
            "63 Batch accuracy: 0.96 Test accuracy: 0.9392\n",
            "64 Batch accuracy: 0.98 Test accuracy: 0.9374\n",
            "65 Batch accuracy: 0.98 Test accuracy: 0.947\n",
            "66 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
            "67 Batch accuracy: 0.98 Test accuracy: 0.9416\n",
            "68 Batch accuracy: 0.94 Test accuracy: 0.9412\n",
            "69 Batch accuracy: 0.92 Test accuracy: 0.9447\n",
            "70 Batch accuracy: 0.94 Test accuracy: 0.9425\n",
            "71 Batch accuracy: 0.96 Test accuracy: 0.9431\n",
            "72 Batch accuracy: 0.96 Test accuracy: 0.9421\n",
            "73 Batch accuracy: 0.96 Test accuracy: 0.9363\n",
            "74 Batch accuracy: 0.96 Test accuracy: 0.9467\n",
            "75 Batch accuracy: 0.98 Test accuracy: 0.9457\n",
            "76 Batch accuracy: 0.94 Test accuracy: 0.9445\n",
            "77 Batch accuracy: 0.98 Test accuracy: 0.9473\n",
            "78 Batch accuracy: 0.96 Test accuracy: 0.9428\n",
            "79 Batch accuracy: 0.98 Test accuracy: 0.9432\n",
            "80 Batch accuracy: 0.96 Test accuracy: 0.9453\n",
            "81 Batch accuracy: 0.98 Test accuracy: 0.9485\n",
            "82 Batch accuracy: 0.96 Test accuracy: 0.9482\n",
            "83 Batch accuracy: 0.96 Test accuracy: 0.9417\n",
            "84 Batch accuracy: 0.96 Test accuracy: 0.9472\n",
            "85 Batch accuracy: 0.94 Test accuracy: 0.9449\n",
            "86 Batch accuracy: 0.98 Test accuracy: 0.9481\n",
            "87 Batch accuracy: 0.98 Test accuracy: 0.9496\n",
            "88 Batch accuracy: 0.96 Test accuracy: 0.9505\n",
            "89 Batch accuracy: 0.94 Test accuracy: 0.9453\n",
            "90 Batch accuracy: 0.98 Test accuracy: 0.9494\n",
            "91 Batch accuracy: 1.0 Test accuracy: 0.952\n",
            "92 Batch accuracy: 0.96 Test accuracy: 0.9466\n",
            "93 Batch accuracy: 0.94 Test accuracy: 0.9496\n",
            "94 Batch accuracy: 0.94 Test accuracy: 0.944\n",
            "95 Batch accuracy: 1.0 Test accuracy: 0.9475\n",
            "96 Batch accuracy: 0.96 Test accuracy: 0.9516\n",
            "97 Batch accuracy: 0.96 Test accuracy: 0.9425\n",
            "98 Batch accuracy: 0.96 Test accuracy: 0.9457\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.9451\n",
            "--- 3077.87 seconds ---\n",
            "0 Batch accuracy: 0.84 Test accuracy: 0.761\n",
            "1 Batch accuracy: 0.86 Test accuracy: 0.8079\n",
            "2 Batch accuracy: 0.88 Test accuracy: 0.8493\n",
            "3 Batch accuracy: 0.84 Test accuracy: 0.823\n",
            "4 Batch accuracy: 0.92 Test accuracy: 0.9038\n",
            "5 Batch accuracy: 0.86 Test accuracy: 0.9098\n",
            "6 Batch accuracy: 0.96 Test accuracy: 0.9185\n",
            "7 Batch accuracy: 0.92 Test accuracy: 0.8956\n",
            "8 Batch accuracy: 0.94 Test accuracy: 0.9088\n",
            "9 Batch accuracy: 0.86 Test accuracy: 0.8886\n",
            "10 Batch accuracy: 0.92 Test accuracy: 0.9233\n",
            "11 Batch accuracy: 0.94 Test accuracy: 0.919\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.917\n",
            "13 Batch accuracy: 0.96 Test accuracy: 0.9239\n",
            "14 Batch accuracy: 0.9 Test accuracy: 0.9272\n",
            "15 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "16 Batch accuracy: 0.94 Test accuracy: 0.9288\n",
            "17 Batch accuracy: 0.96 Test accuracy: 0.9197\n",
            "18 Batch accuracy: 0.9 Test accuracy: 0.9308\n",
            "19 Batch accuracy: 0.88 Test accuracy: 0.9271\n",
            "20 Batch accuracy: 0.96 Test accuracy: 0.9229\n",
            "21 Batch accuracy: 0.92 Test accuracy: 0.9356\n",
            "22 Batch accuracy: 0.94 Test accuracy: 0.9279\n",
            "23 Batch accuracy: 0.96 Test accuracy: 0.9297\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.9353\n",
            "25 Batch accuracy: 0.96 Test accuracy: 0.9387\n",
            "26 Batch accuracy: 1.0 Test accuracy: 0.9218\n",
            "27 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "28 Batch accuracy: 0.94 Test accuracy: 0.9349\n",
            "29 Batch accuracy: 0.96 Test accuracy: 0.9364\n",
            "30 Batch accuracy: 0.98 Test accuracy: 0.9324\n",
            "31 Batch accuracy: 0.94 Test accuracy: 0.935\n",
            "32 Batch accuracy: 1.0 Test accuracy: 0.9399\n",
            "33 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "34 Batch accuracy: 0.98 Test accuracy: 0.935\n",
            "35 Batch accuracy: 0.94 Test accuracy: 0.9466\n",
            "36 Batch accuracy: 0.9 Test accuracy: 0.944\n",
            "37 Batch accuracy: 0.94 Test accuracy: 0.9444\n",
            "38 Batch accuracy: 0.96 Test accuracy: 0.9407\n",
            "39 Batch accuracy: 0.94 Test accuracy: 0.9463\n",
            "40 Batch accuracy: 0.98 Test accuracy: 0.9426\n",
            "41 Batch accuracy: 0.94 Test accuracy: 0.9435\n",
            "42 Batch accuracy: 1.0 Test accuracy: 0.9382\n",
            "43 Batch accuracy: 0.96 Test accuracy: 0.9384\n",
            "44 Batch accuracy: 0.98 Test accuracy: 0.9435\n",
            "45 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "46 Batch accuracy: 0.94 Test accuracy: 0.9378\n",
            "47 Batch accuracy: 0.94 Test accuracy: 0.9443\n",
            "48 Batch accuracy: 1.0 Test accuracy: 0.9482\n",
            "49 Batch accuracy: 0.96 Test accuracy: 0.9454\n",
            "50 Batch accuracy: 0.98 Test accuracy: 0.9465\n",
            "51 Batch accuracy: 0.98 Test accuracy: 0.946\n",
            "52 Batch accuracy: 0.94 Test accuracy: 0.9357\n",
            "53 Batch accuracy: 0.94 Test accuracy: 0.9344\n",
            "54 Batch accuracy: 0.98 Test accuracy: 0.9372\n",
            "55 Batch accuracy: 0.96 Test accuracy: 0.9424\n",
            "56 Batch accuracy: 1.0 Test accuracy: 0.9368\n",
            "57 Batch accuracy: 0.94 Test accuracy: 0.9358\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58 Batch accuracy: 1.0 Test accuracy: 0.9432\n",
            "59 Batch accuracy: 0.96 Test accuracy: 0.9416\n",
            "60 Batch accuracy: 0.9 Test accuracy: 0.9444\n",
            "61 Batch accuracy: 0.98 Test accuracy: 0.9438\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.9366\n",
            "63 Batch accuracy: 0.96 Test accuracy: 0.9392\n",
            "64 Batch accuracy: 0.98 Test accuracy: 0.9374\n",
            "65 Batch accuracy: 0.98 Test accuracy: 0.947\n",
            "66 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
            "67 Batch accuracy: 0.98 Test accuracy: 0.9416\n",
            "68 Batch accuracy: 0.94 Test accuracy: 0.9412\n",
            "69 Batch accuracy: 0.92 Test accuracy: 0.9447\n",
            "70 Batch accuracy: 0.94 Test accuracy: 0.9425\n",
            "71 Batch accuracy: 0.96 Test accuracy: 0.9431\n",
            "72 Batch accuracy: 0.96 Test accuracy: 0.9421\n",
            "73 Batch accuracy: 0.96 Test accuracy: 0.9363\n",
            "74 Batch accuracy: 0.96 Test accuracy: 0.9467\n",
            "75 Batch accuracy: 0.98 Test accuracy: 0.9457\n",
            "76 Batch accuracy: 0.94 Test accuracy: 0.9445\n",
            "77 Batch accuracy: 0.98 Test accuracy: 0.9473\n",
            "78 Batch accuracy: 0.96 Test accuracy: 0.9428\n",
            "79 Batch accuracy: 0.98 Test accuracy: 0.9432\n",
            "80 Batch accuracy: 0.96 Test accuracy: 0.9453\n",
            "81 Batch accuracy: 0.98 Test accuracy: 0.9485\n",
            "82 Batch accuracy: 0.96 Test accuracy: 0.9482\n",
            "83 Batch accuracy: 0.96 Test accuracy: 0.9417\n",
            "84 Batch accuracy: 0.96 Test accuracy: 0.9472\n",
            "85 Batch accuracy: 0.94 Test accuracy: 0.9449\n",
            "86 Batch accuracy: 0.98 Test accuracy: 0.9481\n",
            "87 Batch accuracy: 0.98 Test accuracy: 0.9496\n",
            "88 Batch accuracy: 0.96 Test accuracy: 0.9505\n",
            "89 Batch accuracy: 0.94 Test accuracy: 0.9453\n",
            "90 Batch accuracy: 0.98 Test accuracy: 0.9494\n",
            "91 Batch accuracy: 1.0 Test accuracy: 0.952\n",
            "92 Batch accuracy: 0.96 Test accuracy: 0.9466\n",
            "93 Batch accuracy: 0.94 Test accuracy: 0.9496\n",
            "94 Batch accuracy: 0.94 Test accuracy: 0.944\n",
            "95 Batch accuracy: 1.0 Test accuracy: 0.9475\n",
            "96 Batch accuracy: 0.96 Test accuracy: 0.9516\n",
            "97 Batch accuracy: 0.96 Test accuracy: 0.9425\n",
            "98 Batch accuracy: 0.96 Test accuracy: 0.9457\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.9451\n",
            "--- 3069.41 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L0Ax4Dbc-pM0",
        "outputId": "cf5f17f7-4f2c-4841-b23a-222d09124c13"
      },
      "source": [
        "imported_device_states = import_data_from_csv(filename='learning_curve_vinod.csv')\n",
        "\n",
        "\n",
        "run_MLP_simulation(num_epochs_input = 100, \n",
        "                   hardware_simulation_input = True, \n",
        "                   device_states_input = imported_device_states,\n",
        "                   read_noise_mean_input = 0.0,\n",
        "                   read_noise_stddev_input = 0.1,\n",
        "                   device_variation_stddev_input = 0.0,\n",
        "                   device_stuck_on_prob_input = 0.00,\n",
        "                   device_stuck_off_prob_input = 0.00,\n",
        "                   save_results_input = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 0.72 Test accuracy: 0.6556\n",
            "1 Batch accuracy: 0.6 Test accuracy: 0.5914\n",
            "2 Batch accuracy: 0.7 Test accuracy: 0.7075\n",
            "3 Batch accuracy: 0.5 Test accuracy: 0.5555\n",
            "4 Batch accuracy: 0.82 Test accuracy: 0.7896\n",
            "5 Batch accuracy: 0.76 Test accuracy: 0.7968\n",
            "6 Batch accuracy: 0.8 Test accuracy: 0.776\n",
            "7 Batch accuracy: 0.82 Test accuracy: 0.7581\n",
            "8 Batch accuracy: 0.9 Test accuracy: 0.8142\n",
            "9 Batch accuracy: 0.6 Test accuracy: 0.6915\n",
            "10 Batch accuracy: 0.78 Test accuracy: 0.8267\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.8334\n",
            "12 Batch accuracy: 0.84 Test accuracy: 0.8398\n",
            "13 Batch accuracy: 0.82 Test accuracy: 0.8183\n",
            "14 Batch accuracy: 0.8 Test accuracy: 0.843\n",
            "15 Batch accuracy: 0.92 Test accuracy: 0.846\n",
            "16 Batch accuracy: 0.78 Test accuracy: 0.826\n",
            "17 Batch accuracy: 0.82 Test accuracy: 0.8299\n",
            "18 Batch accuracy: 0.84 Test accuracy: 0.8476\n",
            "19 Batch accuracy: 0.86 Test accuracy: 0.8514\n",
            "20 Batch accuracy: 0.84 Test accuracy: 0.8119\n",
            "21 Batch accuracy: 0.88 Test accuracy: 0.829\n",
            "22 Batch accuracy: 0.78 Test accuracy: 0.8402\n",
            "23 Batch accuracy: 0.86 Test accuracy: 0.8478\n",
            "24 Batch accuracy: 0.96 Test accuracy: 0.8553\n",
            "25 Batch accuracy: 0.76 Test accuracy: 0.8315\n",
            "26 Batch accuracy: 0.88 Test accuracy: 0.8339\n",
            "27 Batch accuracy: 0.88 Test accuracy: 0.8776\n",
            "28 Batch accuracy: 0.84 Test accuracy: 0.8667\n",
            "29 Batch accuracy: 0.86 Test accuracy: 0.8583\n",
            "30 Batch accuracy: 0.86 Test accuracy: 0.874\n",
            "31 Batch accuracy: 0.92 Test accuracy: 0.8726\n",
            "32 Batch accuracy: 0.88 Test accuracy: 0.8804\n",
            "33 Batch accuracy: 0.88 Test accuracy: 0.8656\n",
            "34 Batch accuracy: 0.86 Test accuracy: 0.8395\n",
            "35 Batch accuracy: 0.88 Test accuracy: 0.865\n",
            "36 Batch accuracy: 0.88 Test accuracy: 0.8673\n",
            "37 Batch accuracy: 0.82 Test accuracy: 0.8524\n",
            "38 Batch accuracy: 0.8 Test accuracy: 0.8347\n",
            "39 Batch accuracy: 0.82 Test accuracy: 0.877\n",
            "40 Batch accuracy: 0.92 Test accuracy: 0.8657\n",
            "41 Batch accuracy: 0.82 Test accuracy: 0.881\n",
            "42 Batch accuracy: 0.84 Test accuracy: 0.8589\n",
            "43 Batch accuracy: 0.88 Test accuracy: 0.8753\n",
            "44 Batch accuracy: 0.84 Test accuracy: 0.8859\n",
            "45 Batch accuracy: 0.9 Test accuracy: 0.8712\n",
            "46 Batch accuracy: 0.86 Test accuracy: 0.869\n",
            "47 Batch accuracy: 0.82 Test accuracy: 0.8643\n",
            "48 Batch accuracy: 0.9 Test accuracy: 0.8693\n",
            "49 Batch accuracy: 0.92 Test accuracy: 0.8565\n",
            "50 Batch accuracy: 0.8 Test accuracy: 0.8866\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.8775\n",
            "52 Batch accuracy: 0.9 Test accuracy: 0.8685\n",
            "53 Batch accuracy: 0.74 Test accuracy: 0.8609\n",
            "54 Batch accuracy: 0.86 Test accuracy: 0.8633\n",
            "55 Batch accuracy: 0.92 Test accuracy: 0.8693\n",
            "56 Batch accuracy: 0.82 Test accuracy: 0.871\n",
            "57 Batch accuracy: 0.9 Test accuracy: 0.881\n",
            "58 Batch accuracy: 0.84 Test accuracy: 0.8849\n",
            "59 Batch accuracy: 0.9 Test accuracy: 0.8547\n",
            "60 Batch accuracy: 0.88 Test accuracy: 0.8763\n",
            "61 Batch accuracy: 0.88 Test accuracy: 0.8825\n",
            "62 Batch accuracy: 0.94 Test accuracy: 0.8704\n",
            "63 Batch accuracy: 0.88 Test accuracy: 0.8854\n",
            "64 Batch accuracy: 0.84 Test accuracy: 0.8656\n",
            "65 Batch accuracy: 0.9 Test accuracy: 0.8631\n",
            "66 Batch accuracy: 0.92 Test accuracy: 0.8702\n",
            "67 Batch accuracy: 0.92 Test accuracy: 0.8812\n",
            "68 Batch accuracy: 0.9 Test accuracy: 0.8789\n",
            "69 Batch accuracy: 0.9 Test accuracy: 0.8849\n",
            "70 Batch accuracy: 0.88 Test accuracy: 0.8887\n",
            "71 Batch accuracy: 0.88 Test accuracy: 0.8795\n",
            "72 Batch accuracy: 0.9 Test accuracy: 0.8821\n",
            "73 Batch accuracy: 0.94 Test accuracy: 0.8834\n",
            "74 Batch accuracy: 0.9 Test accuracy: 0.8834\n",
            "75 Batch accuracy: 0.94 Test accuracy: 0.884\n",
            "76 Batch accuracy: 0.92 Test accuracy: 0.8814\n",
            "77 Batch accuracy: 0.9 Test accuracy: 0.8798\n",
            "78 Batch accuracy: 0.92 Test accuracy: 0.8825\n",
            "79 Batch accuracy: 0.92 Test accuracy: 0.886\n",
            "80 Batch accuracy: 0.9 Test accuracy: 0.8741\n",
            "81 Batch accuracy: 0.8 Test accuracy: 0.8845\n",
            "82 Batch accuracy: 0.94 Test accuracy: 0.8958\n",
            "83 Batch accuracy: 0.88 Test accuracy: 0.9004\n",
            "84 Batch accuracy: 0.9 Test accuracy: 0.8872\n",
            "85 Batch accuracy: 0.82 Test accuracy: 0.8762\n",
            "86 Batch accuracy: 0.9 Test accuracy: 0.885\n",
            "87 Batch accuracy: 0.84 Test accuracy: 0.8654\n",
            "88 Batch accuracy: 0.94 Test accuracy: 0.8849\n",
            "89 Batch accuracy: 0.86 Test accuracy: 0.8693\n",
            "90 Batch accuracy: 0.94 Test accuracy: 0.8902\n",
            "91 Batch accuracy: 0.9 Test accuracy: 0.9053\n",
            "92 Batch accuracy: 0.84 Test accuracy: 0.8824\n",
            "93 Batch accuracy: 0.88 Test accuracy: 0.8821\n",
            "94 Batch accuracy: 0.86 Test accuracy: 0.875\n",
            "95 Batch accuracy: 0.98 Test accuracy: 0.8869\n",
            "96 Batch accuracy: 0.92 Test accuracy: 0.8769\n",
            "97 Batch accuracy: 0.82 Test accuracy: 0.8863\n",
            "98 Batch accuracy: 0.88 Test accuracy: 0.8889\n",
            "99 Batch accuracy: 0.94 Test accuracy: 0.8792\n",
            "--- 3071.88 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_rXVNJjl-pM1",
        "outputId": "bd466399-4c62-4e9d-80fe-c636bad5968a"
      },
      "source": [
        "# mock simulation using np.arange for device states\n",
        "run_MLP_simulation(num_epochs_input = 100, \n",
        "                   hardware_simulation_input = False, \n",
        "                   device_states_input = False,\n",
        "                   read_noise_mean_input = 0.0,\n",
        "                   read_noise_stddev_input = 0.0,\n",
        "                   device_variation_stddev_input = 0.0,\n",
        "                   device_stuck_on_prob_input = 0.0,\n",
        "                   device_stuck_off_prob_input = 0.0,\n",
        "                   save_results_input = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 0.88 Test accuracy: 0.8849\n",
            "1 Batch accuracy: 0.86 Test accuracy: 0.9082\n",
            "2 Batch accuracy: 0.92 Test accuracy: 0.9206\n",
            "3 Batch accuracy: 0.96 Test accuracy: 0.9269\n",
            "4 Batch accuracy: 1.0 Test accuracy: 0.9298\n",
            "5 Batch accuracy: 0.98 Test accuracy: 0.9327\n",
            "6 Batch accuracy: 0.96 Test accuracy: 0.9368\n",
            "7 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
            "8 Batch accuracy: 0.96 Test accuracy: 0.944\n",
            "9 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
            "10 Batch accuracy: 0.98 Test accuracy: 0.9467\n",
            "11 Batch accuracy: 0.9 Test accuracy: 0.9497\n",
            "12 Batch accuracy: 0.92 Test accuracy: 0.9499\n",
            "13 Batch accuracy: 0.98 Test accuracy: 0.9533\n",
            "14 Batch accuracy: 0.94 Test accuracy: 0.9533\n",
            "15 Batch accuracy: 0.94 Test accuracy: 0.9553\n",
            "16 Batch accuracy: 1.0 Test accuracy: 0.9558\n",
            "17 Batch accuracy: 0.94 Test accuracy: 0.9566\n",
            "18 Batch accuracy: 0.98 Test accuracy: 0.9581\n",
            "19 Batch accuracy: 0.96 Test accuracy: 0.9574\n",
            "20 Batch accuracy: 0.96 Test accuracy: 0.9579\n",
            "21 Batch accuracy: 0.98 Test accuracy: 0.9595\n",
            "22 Batch accuracy: 0.94 Test accuracy: 0.9607\n",
            "23 Batch accuracy: 0.98 Test accuracy: 0.9607\n",
            "24 Batch accuracy: 0.98 Test accuracy: 0.961\n",
            "25 Batch accuracy: 0.98 Test accuracy: 0.9612\n",
            "26 Batch accuracy: 1.0 Test accuracy: 0.9623\n",
            "27 Batch accuracy: 1.0 Test accuracy: 0.9628\n",
            "28 Batch accuracy: 1.0 Test accuracy: 0.9622\n",
            "29 Batch accuracy: 0.98 Test accuracy: 0.9632\n",
            "30 Batch accuracy: 0.98 Test accuracy: 0.9636\n",
            "31 Batch accuracy: 1.0 Test accuracy: 0.9625\n",
            "32 Batch accuracy: 0.92 Test accuracy: 0.9637\n",
            "33 Batch accuracy: 0.96 Test accuracy: 0.9652\n",
            "34 Batch accuracy: 0.96 Test accuracy: 0.9635\n",
            "35 Batch accuracy: 0.96 Test accuracy: 0.9637\n",
            "36 Batch accuracy: 0.98 Test accuracy: 0.9655\n",
            "37 Batch accuracy: 1.0 Test accuracy: 0.9656\n",
            "38 Batch accuracy: 1.0 Test accuracy: 0.9662\n",
            "39 Batch accuracy: 0.98 Test accuracy: 0.9666\n",
            "40 Batch accuracy: 1.0 Test accuracy: 0.966\n",
            "41 Batch accuracy: 0.98 Test accuracy: 0.9665\n",
            "42 Batch accuracy: 0.96 Test accuracy: 0.9666\n",
            "43 Batch accuracy: 0.98 Test accuracy: 0.9681\n",
            "44 Batch accuracy: 1.0 Test accuracy: 0.9677\n",
            "45 Batch accuracy: 0.98 Test accuracy: 0.9666\n",
            "46 Batch accuracy: 0.96 Test accuracy: 0.9675\n",
            "47 Batch accuracy: 1.0 Test accuracy: 0.9675\n",
            "48 Batch accuracy: 1.0 Test accuracy: 0.9682\n",
            "49 Batch accuracy: 1.0 Test accuracy: 0.9683\n",
            "50 Batch accuracy: 0.98 Test accuracy: 0.9678\n",
            "51 Batch accuracy: 0.96 Test accuracy: 0.968\n",
            "52 Batch accuracy: 0.98 Test accuracy: 0.9683\n",
            "53 Batch accuracy: 1.0 Test accuracy: 0.9681\n",
            "54 Batch accuracy: 1.0 Test accuracy: 0.9697\n",
            "55 Batch accuracy: 0.98 Test accuracy: 0.9691\n",
            "56 Batch accuracy: 1.0 Test accuracy: 0.9692\n",
            "57 Batch accuracy: 1.0 Test accuracy: 0.9697\n",
            "58 Batch accuracy: 0.98 Test accuracy: 0.9706\n",
            "59 Batch accuracy: 0.98 Test accuracy: 0.9696\n",
            "60 Batch accuracy: 0.98 Test accuracy: 0.9705\n",
            "61 Batch accuracy: 1.0 Test accuracy: 0.971\n",
            "62 Batch accuracy: 0.98 Test accuracy: 0.9707\n",
            "63 Batch accuracy: 1.0 Test accuracy: 0.97\n",
            "64 Batch accuracy: 1.0 Test accuracy: 0.9703\n",
            "65 Batch accuracy: 1.0 Test accuracy: 0.9712\n",
            "66 Batch accuracy: 1.0 Test accuracy: 0.972\n",
            "67 Batch accuracy: 0.98 Test accuracy: 0.9716\n",
            "68 Batch accuracy: 1.0 Test accuracy: 0.9721\n",
            "69 Batch accuracy: 1.0 Test accuracy: 0.9718\n",
            "70 Batch accuracy: 1.0 Test accuracy: 0.9716\n",
            "71 Batch accuracy: 1.0 Test accuracy: 0.9714\n",
            "72 Batch accuracy: 1.0 Test accuracy: 0.9721\n",
            "73 Batch accuracy: 1.0 Test accuracy: 0.9718\n",
            "74 Batch accuracy: 0.98 Test accuracy: 0.9717\n",
            "75 Batch accuracy: 0.98 Test accuracy: 0.9722\n",
            "76 Batch accuracy: 1.0 Test accuracy: 0.9725\n",
            "77 Batch accuracy: 1.0 Test accuracy: 0.9724\n",
            "78 Batch accuracy: 1.0 Test accuracy: 0.972\n",
            "79 Batch accuracy: 1.0 Test accuracy: 0.9727\n",
            "80 Batch accuracy: 1.0 Test accuracy: 0.9715\n",
            "81 Batch accuracy: 1.0 Test accuracy: 0.9727\n",
            "82 Batch accuracy: 1.0 Test accuracy: 0.9725\n",
            "83 Batch accuracy: 0.98 Test accuracy: 0.9722\n",
            "84 Batch accuracy: 1.0 Test accuracy: 0.973\n",
            "85 Batch accuracy: 0.98 Test accuracy: 0.9727\n",
            "86 Batch accuracy: 1.0 Test accuracy: 0.9723\n",
            "87 Batch accuracy: 1.0 Test accuracy: 0.9724\n",
            "88 Batch accuracy: 1.0 Test accuracy: 0.9728\n",
            "89 Batch accuracy: 0.98 Test accuracy: 0.9729\n",
            "90 Batch accuracy: 1.0 Test accuracy: 0.9728\n",
            "91 Batch accuracy: 1.0 Test accuracy: 0.9727\n",
            "92 Batch accuracy: 1.0 Test accuracy: 0.9727\n",
            "93 Batch accuracy: 1.0 Test accuracy: 0.9733\n",
            "94 Batch accuracy: 1.0 Test accuracy: 0.9722\n",
            "95 Batch accuracy: 1.0 Test accuracy: 0.9734\n",
            "96 Batch accuracy: 1.0 Test accuracy: 0.9734\n",
            "97 Batch accuracy: 1.0 Test accuracy: 0.9724\n",
            "98 Batch accuracy: 1.0 Test accuracy: 0.9724\n",
            "99 Batch accuracy: 1.0 Test accuracy: 0.973\n",
            "--- 254.00 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bm4RZf9-pM2"
      },
      "source": [
        "# Results\n",
        "\n",
        "## software only (~2 mins , 10 sec)\n",
        "- Learning rate = 0.01\n",
        "0 Batch accuracy: 0.88 Test accuracy: 0.8849\n",
        "1 Batch accuracy: 0.86 Test accuracy: 0.9082\n",
        "2 Batch accuracy: 0.92 Test accuracy: 0.9206\n",
        "3 Batch accuracy: 0.96 Test accuracy: 0.9269\n",
        "4 Batch accuracy: 1.0 Test accuracy: 0.9298\n",
        "5 Batch accuracy: 0.98 Test accuracy: 0.9327\n",
        "6 Batch accuracy: 0.96 Test accuracy: 0.9368\n",
        "7 Batch accuracy: 0.96 Test accuracy: 0.9404\n",
        "8 Batch accuracy: 0.96 Test accuracy: 0.944\n",
        "9 Batch accuracy: 0.98 Test accuracy: 0.9456\n",
        "10 Batch accuracy: 0.98 Test accuracy: 0.9467\n",
        "11 Batch accuracy: 0.9 Test accuracy: 0.9497\n",
        "12 Batch accuracy: 0.92 Test accuracy: 0.9499\n",
        "13 Batch accuracy: 0.98 Test accuracy: 0.9533\n",
        "14 Batch accuracy: 0.94 Test accuracy: 0.9533\n",
        "15 Batch accuracy: 0.94 Test accuracy: 0.9553\n",
        "16 Batch accuracy: 1.0 Test accuracy: 0.9558\n",
        "17 Batch accuracy: 0.94 Test accuracy: 0.9566\n",
        "18 Batch accuracy: 0.98 Test accuracy: 0.9581\n",
        "19 Batch accuracy: 0.96 Test accuracy: 0.9574\n",
        "20 Batch accuracy: 0.96 Test accuracy: 0.9579\n",
        "21 Batch accuracy: 0.98 Test accuracy: 0.9595\n",
        "22 Batch accuracy: 0.94 Test accuracy: 0.9607\n",
        "23 Batch accuracy: 0.98 Test accuracy: 0.9607\n",
        "24 Batch accuracy: 0.98 Test accuracy: 0.961\n",
        "25 Batch accuracy: 0.98 Test accuracy: 0.9612\n",
        "26 Batch accuracy: 1.0 Test accuracy: 0.9623\n",
        "27 Batch accuracy: 1.0 Test accuracy: 0.9628\n",
        "28 Batch accuracy: 1.0 Test accuracy: 0.9622\n",
        "29 Batch accuracy: 0.98 Test accuracy: 0.9632\n",
        "30 Batch accuracy: 0.98 Test accuracy: 0.9636\n",
        "31 Batch accuracy: 1.0 Test accuracy: 0.9625\n",
        "32 Batch accuracy: 0.92 Test accuracy: 0.9637\n",
        "33 Batch accuracy: 0.96 Test accuracy: 0.9652\n",
        "34 Batch accuracy: 0.96 Test accuracy: 0.9635\n",
        "35 Batch accuracy: 0.96 Test accuracy: 0.9637\n",
        "36 Batch accuracy: 0.98 Test accuracy: 0.9655\n",
        "37 Batch accuracy: 1.0 Test accuracy: 0.9656\n",
        "38 Batch accuracy: 1.0 Test accuracy: 0.9662\n",
        "39 Batch accuracy: 0.98 Test accuracy: 0.9666\n",
        "40 Batch accuracy: 1.0 Test accuracy: 0.966\n",
        "41 Batch accuracy: 0.98 Test accuracy: 0.9665\n",
        "42 Batch accuracy: 0.96 Test accuracy: 0.9666\n",
        "43 Batch accuracy: 0.98 Test accuracy: 0.9681\n",
        "44 Batch accuracy: 1.0 Test accuracy: 0.9677\n",
        "45 Batch accuracy: 0.98 Test accuracy: 0.9666\n",
        "46 Batch accuracy: 0.96 Test accuracy: 0.9675\n",
        "47 Batch accuracy: 1.0 Test accuracy: 0.9675\n",
        "48 Batch accuracy: 1.0 Test accuracy: 0.9682\n",
        "49 Batch accuracy: 1.0 Test accuracy: 0.9683\n",
        "--- 140.81 seconds ---\n",
        "\n",
        "## software only (~30 secs)\n",
        "- Learning rate = 0.1\n",
        "0 Batch accuracy: 0.94 Test accuracy: 0.9427\n",
        "1 Batch accuracy: 0.98 Test accuracy: 0.9556\n",
        "2 Batch accuracy: 0.98 Test accuracy: 0.9635\n",
        "3 Batch accuracy: 1.0 Test accuracy: 0.9665\n",
        "4 Batch accuracy: 1.0 Test accuracy: 0.965\n",
        "5 Batch accuracy: 1.0 Test accuracy: 0.9678\n",
        "6 Batch accuracy: 1.0 Test accuracy: 0.9699\n",
        "7 Batch accuracy: 1.0 Test accuracy: 0.9722\n",
        "8 Batch accuracy: 1.0 Test accuracy: 0.9727\n",
        "9 Batch accuracy: 1.0 Test accuracy: 0.9718\n",
        "10 Batch accuracy: 1.0 Test accuracy: 0.9746\n",
        "11 Batch accuracy: 0.98 Test accuracy: 0.9745\n",
        "12 Batch accuracy: 1.0 Test accuracy: 0.9765\n",
        "13 Batch accuracy: 1.0 Test accuracy: 0.9765\n",
        "14 Batch accuracy: 1.0 Test accuracy: 0.9762\n",
        "15 Batch accuracy: 1.0 Test accuracy: 0.9753\n",
        "16 Batch accuracy: 1.0 Test accuracy: 0.9754\n",
        "17 Batch accuracy: 1.0 Test accuracy: 0.977\n",
        "18 Batch accuracy: 1.0 Test accuracy: 0.9756\n",
        "19 Batch accuracy: 1.0 Test accuracy: 0.9768\n",
        "20 Batch accuracy: 1.0 Test accuracy: 0.977\n",
        "\n",
        "## hardware ON, but just discrete states (20) (~2 min, 13 s)\n",
        "0 Batch accuracy: 0.92 Test accuracy: 0.933\n",
        "1 Batch accuracy: 0.92 Test accuracy: 0.938\n",
        "2 Batch accuracy: 0.94 Test accuracy: 0.9418\n",
        "3 Batch accuracy: 0.98 Test accuracy: 0.9431\n",
        "4 Batch accuracy: 0.94 Test accuracy: 0.942\n",
        "5 Batch accuracy: 0.96 Test accuracy: 0.9401\n",
        "6 Batch accuracy: 0.92 Test accuracy: 0.9405\n",
        "7 Batch accuracy: 0.96 Test accuracy: 0.9417\n",
        "8 Batch accuracy: 0.92 Test accuracy: 0.9422\n",
        "9 Batch accuracy: 0.96 Test accuracy: 0.9402\n",
        "10 Batch accuracy: 1.0 Test accuracy: 0.9405\n",
        "11 Batch accuracy: 0.92 Test accuracy: 0.9405\n",
        "12 Batch accuracy: 0.96 Test accuracy: 0.9416\n",
        "13 Batch accuracy: 0.94 Test accuracy: 0.9415\n",
        "14 Batch accuracy: 0.98 Test accuracy: 0.9417\n",
        "15 Batch accuracy: 0.98 Test accuracy: 0.9415\n",
        "16 Batch accuracy: 0.96 Test accuracy: 0.9422\n",
        "17 Batch accuracy: 0.96 Test accuracy: 0.9422\n",
        "18 Batch accuracy: 0.88 Test accuracy: 0.9425\n",
        "19 Batch accuracy: 0.98 Test accuracy: 0.942\n",
        "\n",
        "## hardware ON, but just discrete states (100) ( min,  s)\n",
        "0 Batch accuracy: 0.94 Test accuracy: 0.941\n",
        "1 Batch accuracy: 0.96 Test accuracy: 0.9539\n",
        "2 Batch accuracy: 0.98 Test accuracy: 0.9604\n",
        "3 Batch accuracy: 1.0 Test accuracy: 0.9651\n",
        "4 Batch accuracy: 1.0 Test accuracy: 0.9664\n",
        "5 Batch accuracy: 1.0 Test accuracy: 0.9679\n",
        "6 Batch accuracy: 0.98 Test accuracy: 0.9671\n",
        "7 Batch accuracy: 1.0 Test accuracy: 0.971\n",
        "8 Batch accuracy: 0.98 Test accuracy: 0.9705\n",
        "9 Batch accuracy: 1.0 Test accuracy: 0.9709\n",
        "10 Batch accuracy: 1.0 Test accuracy: 0.9711\n",
        "11 Batch accuracy: 0.98 Test accuracy: 0.971\n",
        "12 Batch accuracy: 1.0 Test accuracy: 0.9704\n",
        "13 Batch accuracy: 0.98 Test accuracy: 0.9706\n",
        "14 Batch accuracy: 1.0 Test accuracy: 0.9729\n",
        "15 Batch accuracy: 1.0 Test accuracy: 0.9724\n",
        "16 Batch accuracy: 1.0 Test accuracy: 0.9736\n",
        "\n",
        "## hardware ON with... (1 min 11s)\n",
        "- 20 states\n",
        "- readnoise = 0.0 +/- 0.2\n",
        "- stuckon = stuckoff = 0.05\n",
        "- device_var = 0.1\n",
        "0 Batch accuracy: 0.4 Test accuracy: 0.3264\n",
        "1 Batch accuracy: 0.44 Test accuracy: 0.4225\n",
        "2 Batch accuracy: 0.52 Test accuracy: 0.4578\n",
        "3 Batch accuracy: 0.52 Test accuracy: 0.4802\n",
        "4 Batch accuracy: 0.64 Test accuracy: 0.6497\n",
        "5 Batch accuracy: 0.58 Test accuracy: 0.6118\n",
        "6 Batch accuracy: 0.46 Test accuracy: 0.551\n",
        "7 Batch accuracy: 0.52 Test accuracy: 0.5524\n",
        "--- 71.81 seconds ---\n",
        "\n",
        "## hardware ON with... (1 min 3s)\n",
        "- 20 states\n",
        "- readnoise = 0.0 +/- 0.05\n",
        "- stuckon = stuckoff = 0.05\n",
        "- device_var = 0.1\n",
        "0 Batch accuracy: 0.88 Test accuracy: 0.8143\n",
        "1 Batch accuracy: 0.94 Test accuracy: 0.8872\n",
        "2 Batch accuracy: 0.92 Test accuracy: 0.8669\n",
        "3 Batch accuracy: 0.78 Test accuracy: 0.8651\n",
        "4 Batch accuracy: 0.96 Test accuracy: 0.8995\n",
        "5 Batch accuracy: 0.9 Test accuracy: 0.9091\n",
        "6 Batch accuracy: 0.92 Test accuracy: 0.8873\n",
        "7 Batch accuracy: 0.88 Test accuracy: 0.8694\n",
        "--- 63.78 seconds ---\n",
        "\n",
        "\n",
        "## hardware ON with... (17 mins)\n",
        "- 100 states\n",
        "- readnoise = 0.0 +/- 0.05\n",
        "- stuckon = stuckoff = 0.05\n",
        "- device_var = 0.1\n",
        "0 Batch accuracy: 0.86 Test accuracy: 0.8376\n",
        "1 Batch accuracy: 0.96 Test accuracy: 0.8902\n",
        "2 Batch accuracy: 0.94 Test accuracy: 0.8869\n",
        "3 Batch accuracy: 0.9 Test accuracy: 0.8921\n",
        "4 Batch accuracy: 0.94 Test accuracy: 0.9196\n",
        "5 Batch accuracy: 0.9 Test accuracy: 0.9166\n",
        "6 Batch accuracy: 0.94 Test accuracy: 0.9174\n",
        "7 Batch accuracy: 0.88 Test accuracy: 0.9134\n",
        "8 Batch accuracy: 0.88 Test accuracy: 0.884\n",
        "9 Batch accuracy: 0.88 Test accuracy: 0.9095\n",
        "10 Batch accuracy: 0.92 Test accuracy: 0.913\n",
        "11 Batch accuracy: 0.86 Test accuracy: 0.909\n",
        "12 Batch accuracy: 0.94 Test accuracy: 0.9265\n",
        "13 Batch accuracy: 0.9 Test accuracy: 0.901\n",
        "14 Batch accuracy: 0.92 Test accuracy: 0.9157\n",
        "15 Batch accuracy: 0.92 Test accuracy: 0.8996\n",
        "16 Batch accuracy: 0.9 Test accuracy: 0.9165\n",
        "17 Batch accuracy: 0.92 Test accuracy: 0.917\n",
        "18 Batch accuracy: 0.96 Test accuracy: 0.926\n",
        "19 Batch accuracy: 0.94 Test accuracy: 0.9229\n",
        "20 Batch accuracy: 0.98 Test accuracy: 0.9061\n",
        "21 Batch accuracy: 0.94 Test accuracy: 0.9252\n",
        "22 Batch accuracy: 0.92 Test accuracy: 0.9188\n",
        "23 Batch accuracy: 0.96 Test accuracy: 0.9229\n",
        "24 Batch accuracy: 0.94 Test accuracy: 0.9326\n",
        "25 Batch accuracy: 0.96 Test accuracy: 0.9163\n",
        "26 Batch accuracy: 0.94 Test accuracy: 0.923\n",
        "27 Batch accuracy: 0.92 Test accuracy: 0.9222\n",
        "28 Batch accuracy: 0.94 Test accuracy: 0.926\n",
        "29 Batch accuracy: 0.98 Test accuracy: 0.9132\n",
        "30 Batch accuracy: 0.96 Test accuracy: 0.9141\n",
        "31 Batch accuracy: 0.94 Test accuracy: 0.9229\n",
        "32 Batch accuracy: 0.88 Test accuracy: 0.9223\n",
        "33 Batch accuracy: 0.9 Test accuracy: 0.8704\n",
        "34 Batch accuracy: 0.9 Test accuracy: 0.9216\n",
        "35 Batch accuracy: 0.92 Test accuracy: 0.9069\n",
        "36 Batch accuracy: 0.94 Test accuracy: 0.9095\n",
        "37 Batch accuracy: 0.92 Test accuracy: 0.9306\n",
        "38 Batch accuracy: 0.9 Test accuracy: 0.9147\n",
        "39 Batch accuracy: 0.94 Test accuracy: 0.9339\n",
        "40 Batch accuracy: 0.96 Test accuracy: 0.9148\n",
        "41 Batch accuracy: 0.96 Test accuracy: 0.8807\n",
        "42 Batch accuracy: 0.92 Test accuracy: 0.8752\n",
        "43 Batch accuracy: 0.92 Test accuracy: 0.9108\n",
        "44 Batch accuracy: 0.92 Test accuracy: 0.9234\n",
        "45 Batch accuracy: 0.94 Test accuracy: 0.9276\n",
        "46 Batch accuracy: 0.94 Test accuracy: 0.9113\n",
        "47 Batch accuracy: 0.9 Test accuracy: 0.914\n",
        "48 Batch accuracy: 0.98 Test accuracy: 0.9277\n",
        "49 Batch accuracy: 0.9 Test accuracy: 0.9335\n",
        "--- 995.70 seconds ---\n",
        "\n",
        "## hardware ON with... (14 mins)\n",
        "- 100 states\n",
        "- readnoise = 0.0 +/- 0.00\n",
        "- stuckon = stuckoff = 0.05\n",
        "- device_var = 0.1\n",
        "\n",
        "0 Batch accuracy: 0.92 Test accuracy: 0.9238\n",
        "1 Batch accuracy: 0.92 Test accuracy: 0.9367\n",
        "2 Batch accuracy: 0.96 Test accuracy: 0.9368\n",
        "3 Batch accuracy: 0.96 Test accuracy: 0.9459\n",
        "4 Batch accuracy: 0.98 Test accuracy: 0.9495\n",
        "5 Batch accuracy: 0.98 Test accuracy: 0.9565\n",
        "6 Batch accuracy: 0.9 Test accuracy: 0.9342\n",
        "7 Batch accuracy: 0.98 Test accuracy: 0.9486\n",
        "8 Batch accuracy: 0.96 Test accuracy: 0.9432\n",
        "9 Batch accuracy: 0.96 Test accuracy: 0.948\n",
        "10 Batch accuracy: 1.0 Test accuracy: 0.9513\n",
        "11 Batch accuracy: 0.94 Test accuracy: 0.9572\n",
        "12 Batch accuracy: 1.0 Test accuracy: 0.9436\n",
        "13 Batch accuracy: 0.92 Test accuracy: 0.9293\n",
        "14 Batch accuracy: 0.96 Test accuracy: 0.9586\n",
        "15 Batch accuracy: 1.0 Test accuracy: 0.9583\n",
        "16 Batch accuracy: 0.98 Test accuracy: 0.9644\n",
        "17 Batch accuracy: 1.0 Test accuracy: 0.9469\n",
        "18 Batch accuracy: 0.9 Test accuracy: 0.9589\n",
        "19 Batch accuracy: 0.98 Test accuracy: 0.9617\n",
        "20 Batch accuracy: 0.96 Test accuracy: 0.9519\n",
        "21 Batch accuracy: 0.94 Test accuracy: 0.9536\n",
        "22 Batch accuracy: 0.96 Test accuracy: 0.9654\n",
        "23 Batch accuracy: 0.98 Test accuracy: 0.9594\n",
        "24 Batch accuracy: 0.98 Test accuracy: 0.9655\n",
        "25 Batch accuracy: 1.0 Test accuracy: 0.9624\n",
        "26 Batch accuracy: 0.96 Test accuracy: 0.9626\n",
        "27 Batch accuracy: 1.0 Test accuracy: 0.958\n",
        "28 Batch accuracy: 1.0 Test accuracy: 0.9532\n",
        "29 Batch accuracy: 1.0 Test accuracy: 0.9704\n",
        "30 Batch accuracy: 0.98 Test accuracy: 0.9576\n",
        "31 Batch accuracy: 1.0 Test accuracy: 0.9594\n",
        "32 Batch accuracy: 0.98 Test accuracy: 0.9676\n",
        "33 Batch accuracy: 1.0 Test accuracy: 0.9568\n",
        "34 Batch accuracy: 1.0 Test accuracy: 0.97\n",
        "35 Batch accuracy: 0.98 Test accuracy: 0.9625\n",
        "36 Batch accuracy: 1.0 Test accuracy: 0.9638\n",
        "37 Batch accuracy: 0.98 Test accuracy: 0.9559\n",
        "38 Batch accuracy: 1.0 Test accuracy: 0.9564\n",
        "39 Batch accuracy: 0.98 Test accuracy: 0.9416\n",
        "40 Batch accuracy: 0.98 Test accuracy: 0.9536\n",
        "41 Batch accuracy: 0.98 Test accuracy: 0.9669\n",
        "42 Batch accuracy: 0.96 Test accuracy: 0.9723\n",
        "43 Batch accuracy: 0.94 Test accuracy: 0.9654\n",
        "44 Batch accuracy: 0.96 Test accuracy: 0.9631\n",
        "45 Batch accuracy: 0.98 Test accuracy: 0.9669\n",
        "46 Batch accuracy: 1.0 Test accuracy: 0.9717\n",
        "47 Batch accuracy: 0.94 Test accuracy: 0.9653\n",
        "48 Batch accuracy: 0.92 Test accuracy: 0.9347\n",
        "49 Batch accuracy: 1.0 Test accuracy: 0.9634\n",
        "--- 875.17 seconds ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx1-XYLt-pM6",
        "outputId": "b20ae610-bbdd-4b1e-e64d-3ff635256a0b"
      },
      "source": [
        "weights_before"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.00363304, -0.16495076, -0.00245212, -0.07429077, -0.0231128 ,\n",
              "       -0.34697148,  0.3885961 ,  0.30065742,  0.22381195, -0.0381159 ,\n",
              "        0.00310417,  0.08535535,  0.17385383,  0.30208942,  0.125374  ,\n",
              "        0.00897076,  0.07525111, -0.29270896, -0.18102363,  0.06648245,\n",
              "       -0.01434219, -0.12452476, -0.23496754,  0.24518795, -0.03454113,\n",
              "        0.09062955,  0.05114428, -0.01170656,  0.31281415,  0.05068856,\n",
              "       -0.07658286, -0.3558821 , -0.02831533,  0.29215947, -0.01215394,\n",
              "        0.05968314,  0.26844263,  0.34673175,  0.00499299,  0.01214231,\n",
              "       -0.26232064, -0.18391348, -0.30076265,  0.08745792,  0.04777403,\n",
              "       -0.30305094,  0.33885407, -0.22074407, -0.01648456,  0.14930668,\n",
              "       -0.10613731, -0.15380175,  0.04764128,  0.1736799 , -0.05961963,\n",
              "       -0.23478894, -0.18598206, -0.05220306, -0.03114807,  0.04526016,\n",
              "       -0.13837756,  0.17210205, -0.11568228, -0.06414939, -0.07839669,\n",
              "       -0.31229424, -0.2896966 ,  0.03144627, -0.03642305, -0.16567579,\n",
              "       -0.00267657,  0.17316784,  0.00085319, -0.09679439, -0.02308435,\n",
              "        0.16394866,  0.01165754,  0.05500915,  0.41136825, -0.05219844,\n",
              "        0.2085114 , -0.07541733, -0.1070393 , -0.01492016,  0.08076164,\n",
              "        0.19692856,  0.13495956, -0.38460404, -0.04849344, -0.08398602,\n",
              "        0.14246887, -0.21932591,  0.06827202, -0.2051726 , -0.17666604,\n",
              "       -0.05961052, -0.10623793, -0.0255966 ,  0.13482282, -0.17893608,\n",
              "        0.09000875, -0.07414095, -0.00147178, -0.3673266 ,  0.08518526,\n",
              "       -0.3438186 , -0.08230692,  0.21740241,  0.13392316, -0.09173317,\n",
              "        0.34868085,  0.06192811,  0.2656322 ,  0.05334428,  0.14007658,\n",
              "        0.2281066 , -0.3195803 ,  0.18029472, -0.19715586, -0.02840302,\n",
              "        0.13200265,  0.1640659 ,  0.08478154,  0.07364658, -0.3171663 ,\n",
              "       -0.11999371,  0.3375892 , -0.00573255, -0.21645758, -0.1885861 ,\n",
              "        0.06762656,  0.218657  ,  0.04552095,  0.3021214 , -0.3293409 ,\n",
              "        0.09390545,  0.15122677, -0.14340286, -0.2328385 , -0.271851  ,\n",
              "        0.11863299,  0.1443909 , -0.32996917, -0.03591542, -0.04326014,\n",
              "        0.07065223, -0.01636403,  0.10281464,  0.05116272,  0.33759323,\n",
              "        0.11180329, -0.27110025,  0.22969015,  0.14410236, -0.30995998,\n",
              "        0.16053978,  0.03548114,  0.17458898,  0.08910751, -0.38312206,\n",
              "        0.20420177, -0.05197211, -0.03291531,  0.20131718, -0.30613896,\n",
              "        0.03595546, -0.12058713,  0.0586332 , -0.11217057,  0.19694291,\n",
              "       -0.17973398, -0.14652523, -0.02917453,  0.26436493, -0.06099458,\n",
              "       -0.11913513, -0.2629287 , -0.19920772, -0.17564017,  0.17017403,\n",
              "       -0.01270065, -0.18048352, -0.16957238,  0.16083811, -0.17279983,\n",
              "       -0.07432749, -0.14277601,  0.30492598,  0.03528479, -0.23299056,\n",
              "       -0.22208396,  0.04902112, -0.09590512,  0.14306852,  0.19182707,\n",
              "       -0.19023907, -0.00794895, -0.12115265, -0.16815071, -0.05340749,\n",
              "       -0.21926986,  0.01095075,  0.0968822 , -0.29020125, -0.02158891,\n",
              "       -0.10390481, -0.20072246,  0.26937482, -0.06449796,  0.08901459,\n",
              "       -0.2591222 ,  0.02090825, -0.02636197, -0.03075243,  0.12408592,\n",
              "       -0.3049613 , -0.13866909,  0.28398675,  0.05102077,  0.01707341,\n",
              "        0.09695832,  0.15275683,  0.02451089, -0.15840153,  0.05042589,\n",
              "       -0.24000023,  0.03005715, -0.2378383 ,  0.10830259,  0.0710686 ,\n",
              "       -0.01086262, -0.00347498,  0.0264352 ,  0.07423522,  0.29181156,\n",
              "       -0.1386595 ,  0.2349203 , -0.0437406 ,  0.19554102,  0.14396428,\n",
              "        0.09926233,  0.07350031, -0.33096662,  0.22308242,  0.17690367,\n",
              "       -0.20106262,  0.00483929, -0.0394629 ,  0.09772117,  0.13834712,\n",
              "       -0.31102046, -0.17354865, -0.12152156, -0.16103064,  0.13679971,\n",
              "        0.07532874,  0.11013555, -0.12267293,  0.09924937, -0.34042314,\n",
              "        0.02201365,  0.27052307,  0.04294404, -0.25432816, -0.19746897,\n",
              "       -0.13402449, -0.23749909,  0.19029996,  0.19579083, -0.26166064,\n",
              "       -0.15412852,  0.17491113, -0.1651232 ,  0.02764448,  0.20055656,\n",
              "       -0.01854535,  0.05692549, -0.08943276,  0.03881437,  0.4106647 ,\n",
              "        0.04369108,  0.03399525,  0.11422517,  0.06415461,  0.3406343 ,\n",
              "        0.0845423 ,  0.05388894,  0.05470451,  0.04703617, -0.09848036,\n",
              "       -0.15698525,  0.07941221, -0.17882024, -0.05065073,  0.05850855,\n",
              "        0.1418438 ,  0.09312636,  0.01073388, -0.05722875, -0.08071069],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "beo9Hh2B-pM9",
        "outputId": "085d09b7-9336-4ba5-ed17-8fcd42374fc2"
      },
      "source": [
        "weights_after"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0. , -0.2, -0. , -0.1, -0. , -0.3,  0.4,  0.3,  0.2, -0. , -0. ,\n",
              "        0.1,  0.2,  0.3,  0.1, -0. ,  0.1, -0.3, -0.2,  0.1, -0. , -0.1,\n",
              "       -0.2,  0.2, -0. ,  0.1,  0.1, -0. ,  0.3,  0.1, -0.1, -0.4, -0. ,\n",
              "        0.3, -0. ,  0.1,  0.3,  0.3, -0. , -0. , -0.3, -0.2, -0.3,  0.1,\n",
              "       -0. , -0.3,  0.3, -0.2, -0. ,  0.1, -0.1, -0.2, -0. ,  0.2, -0.1,\n",
              "       -0.2, -0.2, -0.1, -0. , -0. , -0.1,  0.2, -0.1, -0.1, -0.1, -0.3,\n",
              "       -0.3, -0. , -0. , -0.2, -0. ,  0.2, -0. , -0.1, -0. ,  0.2, -0. ,\n",
              "        0.1,  0.4, -0.1,  0.2, -0.1, -0.1, -0. ,  0.1,  0.2,  0.1, -0.4,\n",
              "       -0. , -0.1,  0.1, -0.2,  0.1, -0.2, -0.2, -0.1, -0.1, -0. ,  0.1,\n",
              "       -0.2,  0.1, -0.1, -0. , -0.4,  0.1, -0.3, -0.1,  0.2,  0.1, -0.1,\n",
              "        0.3,  0.1,  0.3,  0.1,  0.1,  0.2, -0.3,  0.2, -0.2, -0. ,  0.1,\n",
              "        0.2,  0.1,  0.1, -0.3, -0.1,  0.3, -0. , -0.2, -0.2,  0.1,  0.2,\n",
              "       -0. ,  0.3, -0.3,  0.1,  0.2, -0.1, -0.2, -0.3,  0.1,  0.1, -0.3,\n",
              "       -0. , -0. ,  0.1, -0. ,  0.1,  0.1,  0.3,  0.1, -0.3,  0.2,  0.1,\n",
              "       -0.3,  0.2, -0. ,  0.2,  0.1, -0.4,  0.2, -0.1, -0. ,  0.2, -0.3,\n",
              "       -0. , -0.1,  0.1, -0.1,  0.2, -0.2, -0.1, -0. ,  0.3, -0.1, -0.1,\n",
              "       -0.3, -0.2, -0.2,  0.2, -0. , -0.2, -0.2,  0.2, -0.2, -0.1, -0.1,\n",
              "        0.3, -0. , -0.2, -0.2, -0. , -0.1,  0.1,  0.2, -0.2, -0. , -0.1,\n",
              "       -0.2, -0.1, -0.2, -0. ,  0.1, -0.3, -0. , -0.1, -0.2,  0.3, -0.1,\n",
              "        0.1, -0.3, -0. , -0. , -0. ,  0.1, -0.3, -0.1,  0.3,  0.1, -0. ,\n",
              "        0.1,  0.2, -0. , -0.2,  0.1, -0.2, -0. , -0.2,  0.1,  0.1, -0. ,\n",
              "       -0. , -0. ,  0.1,  0.3, -0.1,  0.2, -0. ,  0.2,  0.1,  0.1,  0.1,\n",
              "       -0.3,  0.2,  0.2, -0.2, -0. , -0. ,  0.1,  0.1, -0.3, -0.2, -0.1,\n",
              "       -0.2,  0.1,  0.1,  0.1, -0.1,  0.1, -0.3, -0. ,  0.3, -0. , -0.3,\n",
              "       -0.2, -0.1, -0.2,  0.2,  0.2, -0.3, -0.2,  0.2, -0.2, -0. ,  0.2,\n",
              "       -0. ,  0.1, -0.1, -0. ,  0.4, -0. , -0. ,  0.1,  0.1,  0.3,  0.1,\n",
              "        0.1,  0.1, -0. , -0.1, -0.2,  0.1, -0.2, -0.1,  0.1,  0.1,  0.1,\n",
              "       -0. , -0.1, -0.1], dtype=float32)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYazlfWP-pM-"
      },
      "source": [
        "AlexNet = maxpool lots of layers\n",
        "Vggnet = after alexnet, less intensive convolutions; higher num of params and lower accuracy\n",
        "Resnet = \n",
        "inception = best but lots improve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4oprKOm-pM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}